## kmeans
- 程式1:簡單的function
- 程式2:物件導向
- 程式3:使用scikit-learrn套件
## 程式1:簡單的function ==>chatgpt產生的程式
- PROMPT:請用python程式寫一個簡單的kmeans

```PYTHON
import numpy as np
import matplotlib.pyplot as plt

# K-means 算法實現
def kmeans(X, k, max_iters=100, tol=1e-4):
    # 隨機初始化 K 個中心點
    centers = X[np.random.choice(X.shape[0], k, replace=False)]
    
    for i in range(max_iters):
        # 計算每個點與 K 個中心點的距離
        distances = np.linalg.norm(X[:, np.newaxis] - centers, axis=2)
        
        # 將每個點分配給最近的中心點
        labels = np.argmin(distances, axis=1)
        
        # 計算每個群集的新中心點
        new_centers = np.array([X[labels == j].mean(axis=0) for j in range(k)])
        
        # 檢查中心點是否收斂
        if np.linalg.norm(new_centers - centers) < tol:
            break
        
        centers = new_centers
    
    return centers, labels

# 生成一些隨機數據
np.random.seed(42)
X = np.vstack([np.random.normal(loc, 0.5, (100, 2)) for loc in [(1, 1), (5, 5), (9, 1)]])

# 執行 K-means
k = 3
centers, labels = kmeans(X, k)

# 可視化結果
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='o')
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200)
plt.title("K-means Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```
## 程式2: 請以物件導向方法撰寫kmeans演算法並以iris資料集為範例
- 物件導向 (OOP) 方法實作的 K-Means 演算法，並以 Iris 資料集 作為範例。
- 程式包含：
  - KMeans 類別設計（初始化、分群、更新中心）
  - Iris 資料集載入與視覺化展示
```python
"""
以物件導向方式實作 K-Means 演算法，並以 Iris 資料集為範例
作者：T Ben
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

class KMeans:
    """
    以物件導向方式實作的 KMeans 分群演算法
    """
    def __init__(self, n_clusters=3, max_iter=300, tol=1e-4, random_state=None):
        """
        初始化 KMeans 模型
        :param n_clusters: 分群數
        :param max_iter: 最大迭代次數
        :param tol: 收斂容忍度
        :param random_state: 隨機種子
        """
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.tol = tol
        self.random_state = random_state
        self.centroids = None
        self.labels_ = None

    def _initialize_centroids(self, X):
        """隨機初始化中心點"""
        np.random.seed(self.random_state)
        random_idx = np.random.choice(X.shape[0], self.n_clusters, replace=False)
        return X[random_idx]

    def _assign_clusters(self, X):
        """根據歐氏距離分配每個點到最近的群中心"""
        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)
        return np.argmin(distances, axis=1)

    def _update_centroids(self, X, labels):
        """根據分群結果更新中心點"""
        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(self.n_clusters)])
        return new_centroids

    def fit(self, X):
        """執行 KMeans 分群"""
        self.centroids = self._initialize_centroids(X)

        for i in range(self.max_iter):
            labels = self._assign_clusters(X)
            new_centroids = self._update_centroids(X, labels)

            # 檢查收斂
            if np.all(np.abs(new_centroids - self.centroids) < self.tol):
                print(f"✅ 收斂於第 {i+1} 次迭代")
                break

            self.centroids = new_centroids

        self.labels_ = labels
        return self

    def predict(self, X):
        """預測新資料所屬群"""
        return self._assign_clusters(X)

    def plot_clusters(self, X, title="K-Means Clustering (Iris Example)"):
        """繪製 2D 分群結果"""
        plt.figure(figsize=(7, 5))
        plt.scatter(X[:, 0], X[:, 1], c=self.labels_, cmap='viridis', s=50)
        plt.scatter(self.centroids[:, 0], self.centroids[:, 1], c='red', marker='X', s=200, label='Centroids')
        plt.title(title)
        plt.xlabel("Feature 1")
        plt.ylabel("Feature 2")
        plt.legend()
        plt.show()


# === 主程式區 ===
if __name__ == "__main__":
    # 載入 Iris 資料集
    iris = load_iris()
    X = iris.data[:, :2]  # 取前兩個特徵方便繪圖

    # 標準化處理
    X_scaled = StandardScaler().fit_transform(X)

    # 建立與訓練模型
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X_scaled)

    # 顯示結果
    print("群中心：\n", kmeans.centroids)
    print("群標籤（前10筆）：", kmeans.labels_[:10])

    # 視覺化
    kmeans.plot_clusters(X_scaled)
```
## 程式3:使用scikit-learrn套件
```python
"""
使用 scikit-learn 的 KMeans 對 Iris 資料集進行分群分析
作者：T Ben
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# === 1. 載入資料 ===
iris = load_iris()
X = iris.data
y = iris.target

# === 2. 標準化特徵 ===
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# === 3. 使用 KMeans 進行分群 ===
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
kmeans.fit(X_scaled)

# === 4. 輸出結果 ===
print("KMeans 群中心：")
print(kmeans.cluster_centers_)
print("\n前10筆預測群標籤：", kmeans.labels_[:10])
print("\n實際標籤（前10筆）：", y[:10])

# === 5. 使用 PCA 將資料降維至 2D 以便視覺化 ===
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis', s=50)
plt.scatter(pca.transform(kmeans.cluster_centers_)[:, 0],
            pca.transform(kmeans.cluster_centers_)[:, 1],
            c='red', s=200, marker='X', label='Centroids')
plt.title("K-Means Clustering on Iris Dataset (PCA 2D)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.show()

```

## 程式4:
- 載入並標準化 Iris 資料
- 使用 KMeans 執行分群
- 計算並繪製：
  - Elbow 曲線 (Inertia) → 衡量群內誤差平方和 (WSS)
  - Silhouette Score → 衡量分群品質 (越高越好)
- 視覺化最終分群結果
```python
"""
使用 scikit-learn 的 KMeans + Silhouette Score 分析 Iris 資料集
作者：T Ben
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA

# === 1. 載入並標準化資料 ===
iris = load_iris()
X = iris.data
y = iris.target
X_scaled = StandardScaler().fit_transform(X)

# === 2. 用不同 k 值測試分群效果 ===
inertias = []
sil_scores = []
K_range = range(2, 10)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    sil_scores.append(silhouette_score(X_scaled, kmeans.labels_))

# === 3. 繪製 Elbow 與 Silhouette Score ===
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(K_range, inertias, 'o-', color='blue')
plt.title("Elbow Method (Inertia vs k)")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia (Sum of Squared Distances)")

plt.subplot(1, 2, 2)
plt.plot(K_range, sil_scores, 'o-', color='green')
plt.title("Silhouette Score vs k")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")

plt.tight_layout()
plt.show()

# === 4. 選擇最佳 k (例如 k=3) 並視覺化結果 ===
best_k = 3
kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_scaled)
sil_avg = silhouette_score(X_scaled, labels)

print(f"✅ 最佳群數 k = {best_k}")
print(f"Silhouette Score = {sil_avg:.4f}")
print(f"群中心：\n{kmeans.cluster_centers_}")

# === 5. 降維後視覺化分群結果 ===
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(7, 5))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=50)
plt.scatter(pca.transform(kmeans.cluster_centers_)[:, 0],
            pca.transform(kmeans.cluster_centers_)[:, 1],
            c='red', s=200, marker='X', label='Centroids')
plt.title(f"K-Means (k={best_k}) on Iris Dataset\nSilhouette Score = {sil_avg:.3f}")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.show()

```
