##

## 評估指標 Multilabel ranking metrics
- https://scikit-learn.org/stable/modules/model_evaluation.html#multilabel-ranking-metrics
- 3.4.5. Multilabel ranking metricsv
```python
import numpy as np
from sklearn.metrics import coverage_error, label_ranking_average_precision_score
from sklearn.metrics import label_ranking_loss

# 模擬資料
# y_true: 真實標籤 (multi-label, one-hot encoding)
# y_score: 模型預測分數 (越大表示越可能是正標籤)

y_true = np.array([
    [1, 0, 1],   # 樣本 1 的真實標籤: 類別 0 和 2
    [0, 1, 1],   # 樣本 2 的真實標籤: 類別 1 和 2
    [1, 1, 0]    # 樣本 3 的真實標籤: 類別 0 和 1
])

y_score = np.array([
    [0.8, 0.2, 0.6],   # 模型對樣本 1 的預測分數
    [0.1, 0.9, 0.8],   # 模型對樣本 2 的預測分數
    [0.7, 0.6, 0.2]    # 模型對樣本 3 的預測分數
])

# === Multilabel Ranking Metrics ===
cov_error = coverage_error(y_true, y_score)
lrap = label_ranking_average_precision_score(y_true, y_score)
rank_loss = label_ranking_loss(y_true, y_score)

print(f"Coverage Error: {cov_error:.3f}")
print(f"LRAP (Label Ranking Average Precision): {lrap:.3f}")
print(f"Ranking Loss: {rank_loss:.3f}")

```
## 評估指標 Multiclass ranking metrics
- Python 範例：Multiclass Ranking Metrics
```python
import numpy as np
from sklearn.metrics import top_k_accuracy_score, log_loss, average_precision_score

# 模擬資料
y_true = np.array([0, 2, 1, 2])  # 真實標籤
y_score = np.array([
    [0.7, 0.2, 0.1],  # 樣本 1
    [0.1, 0.3, 0.6],  # 樣本 2
    [0.2, 0.5, 0.3],  # 樣本 3
    [0.1, 0.2, 0.7]   # 樣本 4
])

# === 1. Top-k Accuracy ===
top1_acc = top_k_accuracy_score(y_true, y_score, k=1)
top2_acc = top_k_accuracy_score(y_true, y_score, k=2)

# === 2. Mean Reciprocal Rank (MRR) ===
def mean_reciprocal_rank(y_true, y_score):
    ranks = []
    for true, scores in zip(y_true, y_score):
        sorted_indices = np.argsort(scores)[::-1]  # 排序 (由大到小)
        rank = np.where(sorted_indices == true)[0][0] + 1
        ranks.append(1.0 / rank)
    return np.mean(ranks)

mrr = mean_reciprocal_rank(y_true, y_score)

# === 3. Log Loss ===
ll = log_loss(y_true, y_score)

# === 4. Average Precision (AP, macro) ===
# 需要 one-hot 的真實標籤
n_classes = y_score.shape[1]
y_true_onehot = np.eye(n_classes)[y_true]
ap = average_precision_score(y_true_onehot, y_score, average="macro")

# === 5. One-error ===
# 預測分數最高的類別是否正確
y_pred_top1 = np.argmax(y_score, axis=1)
one_error = np.mean(y_pred_top1 != y_true)

print(f"Top-1 Accuracy: {top1_acc:.3f}")
print(f"Top-2 Accuracy: {top2_acc:.3f}")
print(f"Mean Reciprocal Rank (MRR): {mrr:.3f}")
print(f"Log Loss: {ll:.3f}")
print(f"Average Precision (AP, macro): {ap:.3f}")
print(f"One-error: {one_error:.3f}")
```

```
Top-1 Accuracy: 0.750
Top-2 Accuracy: 1.000
Mean Reciprocal Rank (MRR): 0.875
Log Loss: 0.364
Average Precision (AP, macro): 0.944
One-error: 0.250
```

```

```
## 評估指標 binary ranking metrics
