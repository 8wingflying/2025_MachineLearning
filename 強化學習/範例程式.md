## FrozenLake
- https://aleksandarhaber.com/installation-and-getting-started-with-openai-gym-and-frozen-lake-environment-reinforcement-learning-tutorial/
```python
import gym
import time
env=gym.make("FrozenLake-v1",render_mode='human')
env.reset()
env.render()

print('Initial state of the system')
 
numberOfIterations=30
 
for i in range(numberOfIterations):
    randomAction= env.action_space.sample()
    returnValue=env.step(randomAction)
    env.render()
    print('Iteration: {} and action {}'.format(i+1,randomAction))
    time.sleep(2)
    if returnValue[2]:
        break
 
env.close() 
```

## Q-learning 探索 GridWorld
- https://zhuanlan.zhihu.com/p/1901402523006400212
```python
import numpy as np
import gym
from gym import spaces

# 自訂 4x4 迷宮環境
class GridWorldEnv(gym.Env):
    def __init__(self):
        self.grid_size = 4
        self.start = (0, 0)
        self.goal = (3, 3)
        self.observation_space = spaces.Discrete(self.grid_size ** 2)  # 離散狀態空間
        self.action_space = spaces.Discrete(4)  # 4個動作：上下左右
        self.action_directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 動作對應的座標變化
        self.state = self.start

    def step(self, action):
        x, y = self.state
        dx, dy = self.action_directions[action]
        new_x = max(0, min(x + dx, self.grid_size - 1))  # 邊界限制
        new_y = max(0, min(y + dy, self.grid_size - 1))
        self.state = (new_x, new_y)

        done = (self.state == self.goal)
        reward = 10 if done else -0.1  # 到達終點獎勵，否則小懲罰
        return self._get_state(), reward, done, {}

    def reset(self):
        self.state = self.start
        return self._get_state()

    def _get_state(self):
        return self.state[0] * self.grid_size + self.state[1]  # 將座標轉換為離散狀態

# Q-learning 參數
alpha = 0.1  # 學習率
gamma = 0.99  # 折扣因數
epsilon = 0.1  # 探索率
episodes = 1000

# 初始化環境和 Q 表
env = GridWorldEnv()

# q_table，是用來記錄在 grid 中每個位置，採取某個動作之後，能獲得的獎勵。如 q_table[9, 2] 就代表 index 是 2 的行，index 是 1 的列，執行 action (0, -1) 的時候獎勵。
q_table = np.zeros([env.observation_space.n, env.action_space.n])

# 訓練過程
for episode in range(episodes):
    state = env.reset()
    done = False
    while not done:
        # 探索 vs 利用
        if np.random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()  # 隨機動作（探索），隨機走一步，看一下獎勵
        else:
            action = np.argmax(q_table[state])  # 最優動作（利用），學習到一定程度，採取獎勵最多的一個方向

        # 執行動作並更新 Q 值
        next_state, reward, done, _ = env.step(action)
        # 根據下一步獲取 reward 的最大值，來更新當前 q_table[state, action] 的收益
        q_table[state, action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])
        state = next_state

# q_table # 根據 q_table 可以看出路徑是 0->1->5->6->7->11->15

# 測試訓練結果
state = env.reset()
done = False
print("最優路徑：")
while not done:
    action = np.argmax(q_table[state])
    next_state, _, done, _ = env.step(action)
    print(f"狀態 {state} → 動作 {action} → 狀態 {next_state}")
    state = next_state


```
