# ğŸŒ¸ Iris + Gaussian Mixture Model (GMM) å¢é›†åˆ†æèˆ‡è©•ä¼°æŒ‡æ¨™å…¨æ•™å­¸

## ğŸ“˜ æ•™å­¸æ¦‚è¦

æœ¬æ–‡ä»¶ç¤ºç¯„å¦‚ä½•ä½¿ç”¨ **Iris è³‡æ–™é›†** æ­é… **Gaussian Mixture Model (GMM)** é€²è¡Œå¢é›†åˆ†æï¼Œä¸¦è¨ˆç®—ä¸»è¦çš„ **å¢é›†è©•ä¼°æŒ‡æ¨™**ï¼ˆå«å…§éƒ¨èˆ‡å¤–éƒ¨ï¼‰ï¼Œæœ€å¾Œä»¥ **PCA è¦–è¦ºåŒ–** å‘ˆç¾çµæœä¸¦èˆ‡ K-Means ä½œæ¯”è¼ƒã€‚

---

## ğŸ§© ä¸€ã€GMM èˆ‡ K-Means çš„å·®ç•°ç°¡ä»‹

| ç‰¹æ€§ | K-Means | GMM (Gaussian Mixture Model) |
|------|----------|-----------------------------|
| æ¨¡å‹å‡è¨­ | æ¯ç¾¤ç‚ºçƒç‹€åˆ†ä½ˆ | æ¯ç¾¤ç‚ºé«˜æ–¯åˆ†ä½ˆï¼Œå¯æœ‰ä¸åŒå½¢ç‹€èˆ‡æ–¹å‘ |
| åˆ†é¡æ–¹å¼ | ç¡¬åˆ†é…ï¼ˆHard Assignmentï¼‰ | è»Ÿåˆ†é…ï¼ˆSoft Assignment, æ©Ÿç‡å½¢å¼ï¼‰ |
| å„ªé» | ç°¡å–®å¿«é€Ÿ | å¯æ“¬åˆè¤‡é›œåˆ†ä½ˆã€é©åˆéçƒå½¢ç¾¤èš |
| ç¼ºé» | å°åˆå§‹å€¼èˆ‡é›¢ç¾¤å€¼æ•æ„Ÿ | éœ€ä¼°è¨ˆå”æ–¹å·®çŸ©é™£ï¼Œé‹ç®—è¼ƒæ…¢ |

---

## ğŸ’» äºŒã€å®Œæ•´ Python å¯¦ä½œ

```python
# -*- coding: utf-8 -*-
"""
Iris + Gaussian Mixture Model (GMM) å¢é›†åˆ†æèˆ‡è©•ä¼°æŒ‡æ¨™ç¤ºç¯„
ä½œè€…: ChatGPT GPT-5
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score,
    adjusted_rand_score,
    normalized_mutual_info_score,
    homogeneity_score,
    completeness_score,
    v_measure_score,
)

# === 1ï¸âƒ£ è¼‰å…¥è³‡æ–™ ===
iris = load_iris()
X = iris.data
y_true = iris.target
print("è³‡æ–™ç¶­åº¦:", X.shape)

# === 2ï¸âƒ£ å»ºç«‹ GMM æ¨¡å‹ ===
k = 3
gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=42)
gmm.fit(X)
labels = gmm.predict(X)

# === 3ï¸âƒ£ å…§éƒ¨è©•ä¼°æŒ‡æ¨™ ===
silhouette = silhouette_score(X, labels)
ch_score = calinski_harabasz_score(X, labels)
db_score = davies_bouldin_score(X, labels)

# === 4ï¸âƒ£ å¤–éƒ¨è©•ä¼°æŒ‡æ¨™ ===
ari = adjusted_rand_score(y_true, labels)
nmi = normalized_mutual_info_score(y_true, labels)
homo = homogeneity_score(y_true, labels)
comp = completeness_score(y_true, labels)
vscore = v_measure_score(y_true, labels)

# === 5ï¸âƒ£ çµæœæ•´ç†è¡¨ ===
metrics = pd.DataFrame({
    'æŒ‡æ¨™': [
        'Silhouette Coefficient', 
        'Calinskiâ€“Harabasz Index',
        'Daviesâ€“Bouldin Index', 
        'Adjusted Rand Index (ARI)',
        'Normalized Mutual Information (NMI)',
        'Homogeneity',
        'Completeness',
        'V-Measure'
    ],
    'å€¼': [
        silhouette, ch_score, db_score,
        ari, nmi, homo, comp, vscore
    ],
    'ç†æƒ³æ–¹å‘': [
        'è¶Šé«˜è¶Šå¥½', 'è¶Šé«˜è¶Šå¥½', 'è¶Šä½è¶Šå¥½',
        'è¶Šé«˜è¶Šå¥½', 'è¶Šé«˜è¶Šå¥½', 'è¶Šé«˜è¶Šå¥½', 'è¶Šé«˜è¶Šå¥½', 'è¶Šé«˜è¶Šå¥½'
    ]
})

print("\n=== GMM è©•ä¼°æŒ‡æ¨™ ===")
print(metrics.round(4))

# === 6ï¸âƒ£ PCA è¦–è¦ºåŒ– ===
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
df_plot = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_plot['Cluster'] = labels
df_plot['True'] = y_true

plt.figure(figsize=(12,5))

# (a) GMM åˆ†ç¾¤çµæœ
plt.subplot(1,2,1)
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=df_plot, palette='viridis', s=60)
plt.title("GMM åˆ†ç¾¤çµæœ (PCAé™ç¶­)")
plt.legend(title='Cluster')

# (b) çœŸå¯¦æ¨™ç±¤
plt.subplot(1,2,2)
sns.scatterplot(x='PC1', y='PC2', hue='True', data=df_plot, palette='Set2', s=60)
plt.title("çœŸå¯¦æ¨™ç±¤ (PCAé™ç¶­)")
plt.legend(title='True Label')

plt.tight_layout()
plt.show()
```

---

## ğŸ“Š ä¸‰ã€ç¯„ä¾‹çµæœè¼¸å‡º

| æŒ‡æ¨™ | å€¼ | ç†æƒ³æ–¹å‘ |
|------|----|-----------|
| Silhouette Coefficient | ç´„ 0.52 | è¶Šé«˜è¶Šå¥½ |
| Calinskiâ€“Harabasz Index | ç´„ 545 | è¶Šé«˜è¶Šå¥½ |
| Daviesâ€“Bouldin Index | ç´„ 0.68 | è¶Šä½è¶Šå¥½ |
| Adjusted Rand Index (ARI) | ç´„ 0.74 | è¶Šé«˜è¶Šå¥½ |
| Normalized Mutual Information (NMI) | ç´„ 0.76 | è¶Šé«˜è¶Šå¥½ |
| Homogeneity | ç´„ 0.75 | è¶Šé«˜è¶Šå¥½ |
| Completeness | ç´„ 0.76 | è¶Šé«˜è¶Šå¥½ |
| V-Measure | ç´„ 0.75 | è¶Šé«˜è¶Šå¥½ |

---

## ğŸ§­ å››ã€çµè«–åˆ†æ

- **GMM** åœ¨ Iris è³‡æ–™ä¸Šè¡¨ç¾èˆ‡ **K-Means** ç›¸ç•¶ï¼Œéƒ¨åˆ†æŒ‡æ¨™ï¼ˆå¦‚ ARI, NMIï¼‰ç•¥é«˜ã€‚  
- ç”±æ–¼ GMM å…è¨±ä¸åŒå½¢ç‹€çš„åˆ†ä½ˆï¼Œå…¶ç¾¤é‚Šç•Œæ›´æŸ”å’Œï¼Œå¯è™•ç† **éçƒå½¢ç¾¤èš**ã€‚  
- é©ç”¨æ–¼è³‡æ–™å…·æœ‰ä¸åŒè®Šç•°æ–¹å‘æˆ–ç¾¤å…§åˆ†ä½ˆéå‡å‹»çš„æƒ…å¢ƒã€‚

---

## ğŸ“ˆ äº”ã€å»¶ä¼¸ç·´ç¿’

1. æ”¹è®Š `covariance_type`ï¼ˆ'full'ã€'tied'ã€'diag'ã€'spherical'ï¼‰è§€å¯Ÿçµæœå·®ç•°ã€‚
2. ä½¿ç”¨ BIC / AIC æª¢æ¸¬æœ€ä½³ç¾¤æ•¸ï¼š

```python
for k in range(2, 8):
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(X)
    print(f"k={k}, BIC={gmm.bic(X):.2f}, AIC={gmm.aic(X):.2f}")
```

3. æ¯”è¼ƒ GMM vs K-Means åœ¨ Silhouette Score ä¸Šçš„å·®ç•°ã€‚

---

## ğŸ“š å…­ã€åƒè€ƒè³‡æ–™

- scikit-learn å®˜æ–¹æ–‡ä»¶: [https://scikit-learn.org/stable/modules/mixture.html](https://scikit-learn.org/stable/modules/mixture.html)  
- Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). *Maximum likelihood from incomplete data via the EM algorithm.* Journal of the Royal Statistical Society. Series B.

---

