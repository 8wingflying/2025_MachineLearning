# ğŸŒ¸ Iris è³‡æ–™é›†çš„éç›£ç£å­¸ç¿’æ•™å­¸  
ï¼ˆå« K-Meansãƒ»Hierarchicalãƒ»DBSCANãƒ»PCAãƒ»t-SNEãƒ»UMAPãƒ»LDAãƒ»GMMï¼‰

---

## ğŸ“˜ ä¸€ã€å‰è¨€

éç›£ç£å­¸ç¿’ï¼ˆUnsupervised Learningï¼‰æ–¹æ³•æ–¼ç„¡éœ€æ¨™ç±¤çš„æƒ…æ³ä¸‹ï¼Œç”¨ä¾†æ¢ç´¢è³‡æ–™çš„å…§åœ¨çµæ§‹ã€‚
åœ¨ Iris è³‡æ–™é›†ä¸­ï¼Œæˆ‘å€‘æœƒæ‡‰ç”¨å¤šç¨®åˆ†ç¾¤èˆ‡é™ç¶­æ¼”ç®—æ³•ï¼Œ
ä¸¦åŠ å…¥ **LDA (ç›£ç£å¼å°ç…§)** èˆ‡ **GMM (é«˜æ–¯æ··åˆæ¨¡å‹)** é€²è¡Œå°ç…§æ¯”è¼ƒã€‚

---

## ğŸ“Š äºŒã€è³‡æ–™è¼‰å…¥èˆ‡æ¨™æº–åŒ–
```python
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target   # åƒ…ç”¨æ–¼ LDA å°ç…§

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

---

## ğŸŒ¼ ä¸‰ã€K-Means åˆ†ç¾¤
```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

scores = []
for k in range(2, 7):
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(X_scaled)
    scores.append(silhouette_score(X_scaled, labels))

plt.plot(range(2, 7), scores, marker='o')
plt.title("Silhouette Score vs Cluster Number")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.show()

kmeans = KMeans(n_clusters=3, random_state=42)
labels_km = kmeans.fit_predict(X_scaled)
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels_km, palette='viridis')
plt.title("K-Means Clustering (k=3)")
plt.show()
```

---

## ğŸŒ¿ å››ã€å±¤æ¬¡å¼åˆ†ç¾¤ (Hierarchical Clustering)
```python
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

Z = linkage(X_scaled, method='ward')
plt.figure(figsize=(8, 5))
dendrogram(Z)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Samples")
plt.ylabel("Distance")
plt.show()

labels_h = fcluster(Z, 3, criterion='maxclust')
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels_h, palette='rainbow')
plt.title("Hierarchical Clustering (Ward linkage)")
plt.show()
```

---

## ğŸŒ» äº”ã€DBSCAN åˆ†ç¾¤
```python
from sklearn.cluster import DBSCAN

db = DBSCAN(eps=0.8, min_samples=5)
labels_db = db.fit_predict(X_scaled)
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels_db, palette='coolwarm')
plt.title("DBSCAN Clustering")
plt.show()
```

---

## ğŸŒº å…­ã€PCA ç·šæ€§é™ç¶­
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels_km, palette='viridis')
plt.title("K-Means on PCA-reduced Iris Data")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.show()

print("\u89e3\u91cb\u8b8a\u7570\u6bd4:", pca.explained_variance_ratio_)
```

---

## ğŸŒˆ ä¸ƒã€t-SNE éç·šæ€§é™ç¶­
```python
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels_km, palette='Spectral')
plt.title("t-SNE Visualization of Iris Clusters")
plt.xlabel("t-SNE 1")
plt.ylabel("t-SNE 2")
plt.show()
```

---

## ğŸŒ¸ å…«ã€UMAP éç·šæ€§é™ç¶­
```python
import umap

umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
X_umap = umap_model.fit_transform(X_scaled)

sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=labels_km, palette='cool')
plt.title("UMAP Visualization of Iris Clusters")
plt.xlabel("UMAP 1")
plt.ylabel("UMAP 2")
plt.show()
```

---

## ğŸŒ¼ ä¹ã€LDA (ç›£ç£å¼) é™ç¶­å°ç…§
```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X_scaled, y)

sns.scatterplot(x=X_lda[:, 0], y=X_lda[:, 1], hue=iris.target, palette='Set2')
plt.title("LDA Projection of Iris (Supervised Reference)")
plt.xlabel("LD1")
plt.ylabel("LD2")
plt.show()
```

---

## ğŸŒ· åã€GMM (é«˜æ–¯æ··åˆæ¨¡å‹)
```python
from sklearn.mixture import GaussianMixture
from sklearn.metrics import adjusted_rand_score

gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
labels_gmm = gmm.fit_predict(X_scaled)

sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels_gmm, palette='viridis')
plt.title("Gaussian Mixture Model Clustering")
plt.show()

ari = adjusted_rand_score(y, labels_gmm)
print(f"Adjusted Rand Index (\u8207\u771f\u5be6\u6a19\u7c64\u76f8\u4f3c\u5ea6): {ari:.3f}")
```

---

## ğŸŒº åä¸€ã€åœ¨é™ç¶­ç©ºé–“ (PCAã€UMAP) ä¸Šå¥—ç”¨ GMM åˆ†ç¾¤

### âœª 1. PCA + GMM å¯è¦–åŒ–
```python
gmm_pca = GaussianMixture(n_components=3, random_state=42)
labels_gmm_pca = gmm_pca.fit_predict(X_pca)

sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels_gmm_pca, palette='Set1')
plt.title("GMM Clustering on PCA-reduced Iris Data")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.show()
```

### âœª 2. UMAP + GMM å¯è¦–åŒ–
```python
gmm_umap = GaussianMixture(n_components=3, random_state=42)
labels_gmm_umap = gmm_umap.fit_predict(X_umap)

sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=labels_gmm_umap, palette='Set2')
plt.title("GMM Clustering on UMAP-reduced Iris Data")
plt.xlabel("UMAP 1")
plt.ylabel("UMAP 2")
plt.show()
```

---

## ğŸŒ» åäºŒã€æ–¹æ³•æ¯”è¼ƒç¸½è¡¨

| æ–¹æ³• | é¡å‹ | æ˜¯å¦éœ€æ¨™ç±¤ | ç‰¹æ€§ | åˆ†ç¾¤å½¢ç‹€ | å„ªé» | ç¼ºé» |
|------|------|-----------|------|-----------|------|------|
| K-Means | åˆ†ç¾¤ | å¦ | ä»¥ä¸­å¿ƒé»æœ€å°åŒ–ç¾¤å…§è·é›¢ | çƒç‹€ | ç°¡å–®ã€å¿«é€Ÿ | å°åˆå§‹é»æ•æ„Ÿ |
| Hierarchical | åˆ†ç¾¤ | å¦ | ä¾è·é›¢åˆä½µ | ä»»æ„ | å¯è¦–åŒ–å±¤æ¬¡ | å¤§æ¨£æœ¬æ•ˆç‡ä½ |
| DBSCAN | åˆ†ç¾¤ | å¦ | å¯†åº¦åŸºç¤ | ä»»æ„å½¢ | è‡ªå‹•é›¢ç¾¤é» | åƒæ•¸æ•æ„Ÿ |
| GMM | åˆ†ç¾¤ | å¦ | æ¨¡æ“¬æ¨¡å‹ | æ©¢åœ“å½¢ | å¯è¼¸å‡ºæ©Ÿç‡ | æ˜“é™·å±€éƒ¨æœ€å°å€¼ |
| PCA | é™ç¶­ | å¦ | ç·šæ€§ | å…¨å±€ | ç­è§£è³‡æ–™è®Šç•° | ç„¡æ³•åˆ†ç¾¤ |
| t-SNE | é™ç¶­ | å¦ | éç·šæ€§ | å±€éƒ¨ | åˆ†ç¾¤è¦–è¦ºæ¸…æ¥š | ä¸ä¿å…¨å±€çµæ§‹ |
| UMAP | é™ç¶­ | å¦ | éç·šæ€§ | å±€éƒ¨ + å…¨å±€ | å¿«é€Ÿã€ç©©å®š | åƒæ•¸æ•æ„Ÿ |
| LDA | é™ç¶­ | æ˜¯ | é¡åˆ¥æœ€å¤§åŒ– | ç·šæ€§ | é¡åˆ¥åˆ†é›¢æ¸…æ¥š | éœ€æ¨™ç±¤ |

---

## ğŸŒ¼ åä¸‰ã€å»¶ä¼¸æŒ‘æˆ°
1. æ¯”è¼ƒ GMM èˆ‡ K-Means åœ¨ **ARI / NMI** æŒ‡æ¨™ä¸‹çš„å·®ç•°ã€‚  
2. å°‡ GMM ç¾¤çš„ç­‰é«˜ç·šç·š (Gaussian Contour) ç¹ªåˆ»åœ¨ PCA ç©ºé–“ä¸­ã€‚  
3. åœ¨ t-SNE é™ç¶­ç©ºé–“ä¸­å˜—è©¦ GMM ä¸¦æ¯”è¼ƒç¾¤é«”ç©©å®šæ€§ã€‚  
4. ä½¿ç”¨ GMM çš„æ©Ÿç‡è¼¸å‡ºé€²è¡Œ soft label è¨“ç·´ã€‚  
5. å˜—è©¦ Bayesian GMM (ä½¿ç”¨ `BayesianGaussianMixture`) è‡ªå‹•æ¨æ–·ç¾¤æ•¸ã€‚  

---
```

