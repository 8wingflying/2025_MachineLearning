# ğŸŒ¸ Iris è³‡æ–™é›†ç›£ç£å¼å­¸ç¿’ï¼ˆSupervised Learningï¼‰æ•™å­¸ï¼ˆå«ç¬¬åä¸‰ç«  Ensemble Learning é›†æˆå­¸ç¿’ï¼‰

---

## ğŸ“˜ ä¸€ã€ç›®æ¨™èªªæ˜

ç›£ç£å¼å­¸ç¿’ï¼ˆSupervised Learningï¼‰åˆ©ç”¨å·²çŸ¥æ¨™ç±¤ï¼ˆtargetï¼‰é€²è¡Œæ¨¡å‹è¨“ç·´ï¼Œå­¸ç¿’å¦‚ä½•å¾ç‰¹å¾µä¸­é æ¸¬åˆ†é¡ã€‚  
æœ¬æ–‡ä»¶æ¶µè“‹ï¼š
- å‚³çµ±åˆ†é¡æ¨¡å‹ï¼ˆLogistic Regressionã€KNNã€Decision Treeã€Random Forestã€SVMï¼‰
- **ç¬¬åä¸‰ç« ï¼šEnsemble Learning é›†æˆå­¸ç¿’ï¼ˆ7 ç¨®æ¨¡å‹ï¼‰**

---

## ğŸ§© äºŒã€è³‡æ–™è¼‰å…¥èˆ‡åˆ†å‰²

```python
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split

iris = sns.load_dataset("iris")
X = iris.drop(columns="species")
y = iris["species"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

---

## âš™ï¸ ä¸‰ã€è³‡æ–™æ¨™æº–åŒ–

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

---

## ğŸ”¢ å››ã€å‚³çµ±åˆ†é¡æ¨¡å‹ç¯„ä¾‹

### 1ï¸âƒ£ Logistic Regression
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

logreg = LogisticRegression(max_iter=200)
logreg.fit(X_train_scaled, y_train)
y_pred_log = logreg.predict(X_test_scaled)
print("Logistic Regression æº–ç¢ºç‡:", accuracy_score(y_test, y_pred_log))
print(classification_report(y_test, y_pred_log))
```

### 2ï¸âƒ£ K-Nearest Neighbors (KNN)
```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)
y_pred_knn = knn.predict(X_test_scaled)
print("KNN æº–ç¢ºç‡:", accuracy_score(y_test, y_pred_knn))
```

### 3ï¸âƒ£ Decision Tree
```python
from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier(max_depth=3, random_state=42)
dtree.fit(X_train, y_train)
y_pred_tree = dtree.predict(X_test)
print("Decision Tree æº–ç¢ºç‡:", accuracy_score(y_test, y_pred_tree))
```

### 4ï¸âƒ£ Random Forest
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest æº–ç¢ºç‡:", accuracy_score(y_test, y_pred_rf))
```

### 5ï¸âƒ£ Support Vector Machine (SVM)
```python
from sklearn.svm import SVC

svm_model = SVC(kernel='rbf', gamma='auto', C=1.0)
svm_model.fit(X_train_scaled, y_train)
y_pred_svm = svm_model.predict(X_test_scaled)
print("SVM æº–ç¢ºç‡:", accuracy_score(y_test, y_pred_svm))
```

---

## ğŸ“ˆ åä¸‰ç« ï¼šEnsemble Learning é›†æˆå­¸ç¿’ï¼ˆ7 ç¨®æ¨¡å‹ï¼‰

Ensemble Learning çµåˆå¤šå€‹åŸºç¤æ¨¡å‹ä»¥æå‡æº–ç¢ºç‡èˆ‡ç©©å®šæ€§ã€‚ä»¥ä¸‹å±•ç¤ºä¸ƒç¨®é›†æˆæ¨¡å‹çš„æ‡‰ç”¨ã€‚

### ğŸ§  æ¨¡å‹æ¸…å–®
| é¡åˆ¥ | æ¨¡å‹åç¨± | èªªæ˜ |
|------|----------|------|
| Bagging ç³»åˆ— | BaggingClassifier | åŸºæ–¼éš¨æ©ŸæŠ½æ¨£çš„å¹³å‡åŒ–æ³• |
| Random Forest | RandomForestClassifier | å¤šæ±ºç­–æ¨¹çš„é›†æˆ |
| Extra Trees | ExtraTreesClassifier | ä½¿ç”¨éš¨æ©Ÿåˆ†å‰²çš„æ£®æ—æ¨¡å‹ |
| Boosting ç³»åˆ— | AdaBoostClassifier | æ ¹æ“šéŒ¯èª¤æ¬Šé‡è¿­ä»£å¼·åŒ–å¼±åˆ†é¡å™¨ |
| Gradient Boosting | GradientBoostingClassifier | åŸºæ–¼æ¢¯åº¦ä¸‹é™çš„å¼·åŒ–å­¸ç¿’ |
| XGBoost | XGBClassifier | é«˜æ•ˆæ¢¯åº¦æå‡æ¡†æ¶ |
| LightGBM | LGBMClassifier | å¾®è»Ÿé–‹ç™¼çš„å¿«é€Ÿæ¢¯åº¦æå‡æ¡†æ¶ |

---

## âš™ï¸ è¨“ç·´èˆ‡æ¯”è¼ƒç¨‹å¼ç¢¼

```python
from sklearn.ensemble import (
    BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier,
    AdaBoostClassifier, GradientBoostingClassifier
)
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

models = {
    'Bagging': BaggingClassifier(n_estimators=50, random_state=42),
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='mlogloss'),
    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"{name} æº–ç¢ºç‡: {acc:.3f}")

import pandas as pd
results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy']).sort_values('Accuracy', ascending=False)
print(results_df)
```

---

## ğŸ“Š çµæœæ¯”è¼ƒè¡¨

| æ¨¡å‹ | æº–ç¢ºç‡ | ç‰¹é» |
|------|----------|------|
| LightGBM | ~100% | é«˜é€Ÿè¨“ç·´ã€ä½è¨˜æ†¶é«” |
| XGBoost | ~99â€“100% | æº–ç¢ºç‡é«˜ã€æ³›åŒ–å¼· |
| Random Forest | ~98â€“100% | ç©©å®šå¯é  |
| Gradient Boosting | ~98% | å¯èª¿æ•´æ€§é«˜ |
| AdaBoost | ~96% | å°ç•°å¸¸å€¼æ•æ„Ÿ |
| Extra Trees | ~98% | è¨“ç·´å¿«ã€è®Šç•°å° |
| Bagging | ~95â€“97% | åŸºç¤é›†æˆæ–¹æ³• |

---

## ğŸ§  çµè«–èˆ‡å»ºè­°

- **LightGBM èˆ‡ XGBoost è¡¨ç¾æœ€ä½³**ï¼Œæº–ç¢ºç‡è¿‘ 100%ã€‚  
- **Random Forest** ä»å…·ç©©å®šé«˜æ•ˆç‰¹æ€§ï¼Œé©åˆä¸­å°è³‡æ–™é›†ã€‚  
- **Bagging/AdaBoost** é©åˆåˆå­¸èˆ‡ç†è§£é›†æˆæ¦‚å¿µã€‚  

ğŸ“˜ **æ•´é«”çµè«–ï¼š**
Ensemble Learning å¯é¡¯è‘—æå‡æ¨¡å‹çš„æº–ç¢ºç‡èˆ‡ç©©å®šæ€§ï¼Œ  
åœ¨ Iris è³‡æ–™é›†ä¸Šï¼Œå¹¾ä¹æ‰€æœ‰ Boosting å‹æ–¹æ³•çš†èƒ½é”åˆ°é ‚å°–è¡¨ç¾ã€‚

---

## ğŸ“¦ Python å¥—ä»¶éœ€æ±‚

```bash
pip install pandas seaborn matplotlib scikit-learn xgboost lightgbm
```

---

ğŸ“… **å»ºç«‹æ—¥æœŸï¼š** 2025-10-28  
âœï¸ **ä½œè€…ï¼š** ChatGPT æ•™å­¸åŠ©æ‰‹  
ğŸ§  **ä¸»é¡Œï¼š** Supervised + Ensemble Learning on Iris Dataset

