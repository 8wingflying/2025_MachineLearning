# Linear Regression å…¨è§£æ³•å®Œæ•´æ•™æï¼ˆ18 ç¨®ï¼‰

æœ¬æ•™ææ•´ç†ã€Œç·šæ€§å›æ­¸ã€çš„æ‰€æœ‰ä¸»æµæ±‚è§£æ–¹å¼ï¼ŒåŒ…å«ï¼š

- è§£æè§£ï¼ˆClosed-formï¼‰
- æ•¸å€¼ç·šæ€§ä»£æ•¸è§£æ³•ï¼ˆSVD / QR / Choleskyï¼‰
- è¿­ä»£å¼æ±‚è§£ï¼ˆGD / SGD / CG / LBFGSï¼‰
- æ­£å‰‡åŒ–å›æ­¸ï¼ˆL2 / L1 / Elastic Netï¼‰
- Robust å›æ­¸ï¼ˆHuber / RANSACï¼‰
- çµ±è¨ˆé™ç¶­å›æ­¸ï¼ˆPCA Regression / PLSï¼‰

ä¸¦å« **å…¬å¼ã€ç”¨é€”ã€æ•¸å€¼ç©©å®šæ€§ã€Python ç¨‹å¼ç¢¼ç¤ºä¾‹**ã€‚

---

# ç›®éŒ„

1. å•é¡Œå®šç¾©  
2. è§£æè§£æ³•ï¼ˆClosed-formï¼‰  
3. æ•¸å€¼åˆ†è§£æ³•ï¼ˆSVD / QR / Choleskyï¼‰  
4. å¤§è¦æ¨¡è³‡æ–™çš„è¿­ä»£æ³•ï¼ˆGD / SGD / Mini-batch / CGï¼‰  
5. æ­£å‰‡åŒ–ç·šæ€§å›æ­¸ï¼ˆRidge / Lasso / Elastic Netï¼‰  
6. çµ±è¨ˆ/é™ç¶­æ–¹æ³•ï¼ˆPCA Regression / PLSï¼‰  
7. Robust å›æ­¸ï¼ˆHuber / RANSACï¼‰  
8. æ©Ÿå™¨å­¸ç¿’å¼å„ªåŒ–ï¼ˆNewton / LBFGSï¼‰  
9. Python ç¨‹å¼åŒ¯ç¸½  
10. ç¸½æ¯”è¼ƒè¡¨  

---

# 1ï¸âƒ£ å•é¡Œå®šç¾©ï¼ˆLinear Regressionï¼‰

çµ¦å®šï¼š

$$
y = X\beta + \epsilon
$$

æœ€å°äºŒä¹˜è§£ï¼ˆLeast Squaresï¼‰æ‰¾ï¼š

$$
\beta^\* = \arg\min_\beta \|X\beta - y\|_2^2
$$

---

# 2ï¸âƒ£ è§£æè§£æ³•ï¼ˆClosed-form Solutionsï¼‰

## 2.1 Normal Equation

$$
\beta = (X^TX)^{-1}X^Ty
$$

âš  ç¼ºé»ï¼š  
- å¿…é ˆ invert \(X^TX\)
- multicollinearity æ™‚æœƒæ•¸å€¼çˆ†ç‚¸  
- ä¸é©åˆé«˜ç¶­æˆ–å¤§å‹è³‡æ–™

---

## 2.2 Moore-Penrose Pseudo-inverse

$$
\beta = X^{+} y
$$

`Xâº = V Î£âº Uáµ€`ï¼ˆSVD è¨ˆç®—ï¼‰  
âš¡ æœ€ç©©å®šçš„ analytical è§£ã€‚

---

## 2.3 SVD è§£ï¼ˆæœ€ç©©å®šï¼‰

$$
X = U\Sigma V^T
$$

$$
\beta^\* = V\Sigma^{+}U^Ty
$$

---

# 3ï¸âƒ£ æ•¸å€¼ç·šæ€§ä»£æ•¸è§£æ³•ï¼ˆé©åˆä¸­å‹è³‡æ–™ï¼‰

## 3.1 QR Decomposition

$$
X = QR
$$

$$
\beta = R^{-1}Q^Ty
$$

ğŸ¯ å¯¦å‹™å¸¸ç”¨ï¼Œæ¯” Normal Equation ç©©å®šã€‚

---

## 3.2 Cholesky Decompositionï¼ˆåƒ… SPDï¼‰

$$
X^TX = LL^T
$$

$$
LL^T\beta = X^Ty
$$

âš  è‹¥ \(X\) æœ‰å…±ç·šæ€§ â†’ ç„¡æ³•ä½¿ç”¨ã€‚

---

## 3.3 LU Decompositionï¼ˆé–“æ¥æ–¹å¼ï¼‰

è§£ normal equationï¼š

$$
(X^TX)\beta = X^Ty
$$

ä¸å¦‚ QR / SVD ç©©å®šã€‚

---

# 4ï¸âƒ£ å¤§è¦æ¨¡è³‡æ–™ï¼ˆIterative Methodsï¼‰

## 4.1 Gradient Descentï¼ˆGDï¼‰

æ›´æ–°å¼ï¼š

$$
\beta := \beta - \alpha X^T(X\beta - y)
$$

---

## 4.2 Stochastic Gradient Descentï¼ˆSGDï¼‰

æ¯æ¬¡æ›´æ–°ä¸€ç­†ï¼š

$$
\beta := \beta - \alpha (x_i^T\beta - y_i)x_i
$$

ğŸ¯ é©åˆæ·±åº¦å­¸ç¿’ã€å¤§å‹è³‡æ–™ã€‚

---

## 4.3 Mini-Batch Gradient Descent

æ¯” GD å¿«ï¼Œæ¯” SGD æ›´ç©©ã€‚

---

## 4.4 Conjugate Gradientï¼ˆCGï¼‰

è§£ï¼š

$$
(X^TX)\beta = X^Ty
$$

éå¸¸é©åˆï¼šã€Œå¤§å‹ã€ç¨€ç–ã€è³‡æ–™ã€‚

---

# 5ï¸âƒ£ æ­£å‰‡åŒ– Linear Regression

## 5.1 Ridge Regressionï¼ˆL2 æ­£å‰‡åŒ–ï¼‰

$$
\beta = (X^TX + \lambda I)^{-1}X^Ty
$$

ğŸ’¡ å¯ç”¨ï¼š
- SVD
- QR
- Cholesky  
æ±‚è§£ã€‚

---

## 5.2 Lassoï¼ˆL1ï¼‰

$$
\beta = \arg\min_\beta \|X\beta - y\|^2 + \lambda \|\beta\|_1
$$

ç„¡é–‰å¼è§£ â†’ Coordinate Descentã€‚

---

## 5.3 Elastic Netï¼ˆL1 + L2ï¼‰

$$
\|X\beta - y\|^2 + \lambda_1\|\beta\|_1 + \lambda_2\|\beta\|_2^2
$$

---

# 6ï¸âƒ£ çµ±è¨ˆé™ç¶­æ³• Regression

## 6.1 PCA Regression

1. å° X åš PCA  
2. å–å‰ k PCs  
3. åœ¨ä½ç¶­ç©ºé–“å›æ­¸

å¯è§£æ±º multicollinearityã€‚

---

## 6.2 PLSï¼ˆPartial Least Squaresï¼‰

åŒæ™‚è€ƒæ…®ï¼š
- X çš„è®Šç•°
- èˆ‡ y çš„ç›¸é—œæ€§

å¸¸ç”¨æ–¼åŒ–å­¸å…‰è­œã€ç”Ÿé†«è³‡æ–™ã€‚

---

# 7ï¸âƒ£ Robust Regression

## 7.1 Huber Regression

Lossï¼š

$$
L_\delta(r)=
\begin{cases}
\frac{1}{2}r^2, & |r|\le\delta\\
\delta(|r| - \frac{1}{2}\delta), & |r|>\delta
\end{cases}
$$

é©åˆå« outlierã€‚

---

## 7.2 RANSAC Regression

éš¨æ©ŸæŠ½å­é›† â†’ ä¼°è¨ˆæ¨¡å‹  
é©åˆé›œè¨Šéå¸¸é«˜çš„è³‡æ–™ã€‚

---

# 8ï¸âƒ£ æ©Ÿå™¨å­¸ç¿’å„ªåŒ–ï¼ˆML Optimization Methodsï¼‰

## 8.1 Newtonâ€™s Method

$$
\beta_{k+1} = \beta_k - H^{-1}\nabla J
$$

---

## 8.2 Quasi-Newtonï¼ˆL-BFGSï¼‰

æ·±åº¦å­¸ç¿’å¸¸ç”¨  
é©åˆä¸­å¤§å‹è³‡æ–™ã€‚

---

