# 叢集演算法
```python
# ============================================================
# 18 種叢集演算法分析信用卡詐欺 (Kaggle creditcard.csv)
# 已移除 Fuzzy C-Means & LDA 相關程式
# ============================================================

import numpy as np
import pandas as pd

# -----------------------------
# 1. 讀取資料與前處理
# -----------------------------
data_path = "creditcard.csv"
df = pd.read_csv(data_path)

print("資料筆數：", df.shape)
print(df["Class"].value_counts())

from sklearn.preprocessing import StandardScaler

y = df["Class"].values
X = df.drop(columns=["Class"])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -----------------------------
# 2. PCA 降維加速
# -----------------------------
from sklearn.decomposition import PCA

pca = PCA(n_components=10, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# -----------------------------
# 3. 取樣加速
# -----------------------------
N_SAMPLE = 30000
if len(df) > N_SAMPLE:
    df_sample = df.sample(n=N_SAMPLE, random_state=42)
    y_sample = df_sample["Class"].values
    X_sample_scaled = scaler.transform(df_sample.drop(columns=["Class"]))
    X_sample_pca = pca.transform(X_sample_scaled)
else:
    X_sample_pca = X_pca
    y_sample = y

# -----------------------------
# 4. 定義 18 種叢集演算法（移除 FCM）
# -----------------------------
from sklearn.cluster import (
    KMeans, MiniBatchKMeans, AgglomerativeClustering, Birch,
    SpectralClustering, DBSCAN, OPTICS, MeanShift, AffinityPropagation
)
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture

try:
    from sklearn_extra.cluster import KMedoids
    SKLEARN_EXTRA_OK = True
except:
    SKLEARN_EXTRA_OK = False
    print("未安裝 scikit-learn-extra，KMedoids 略過")

try:
    import hdbscan
    HDBSCAN_OK = True
except:
    HDBSCAN_OK = False
    print("未安裝 hdbscan，HDBSCAN 略過")


def make_algorithms():
    algos = []

    # 1-4 KMeans 系列
    algos.append(("KMeans_k2", KMeans(n_clusters=2, random_state=42)))
    algos.append(("KMeans_k4", KMeans(n_clusters=4, random_state=42)))
    algos.append(("MiniBatchKMeans_k2", MiniBatchKMeans(n_clusters=2, random_state=42)))
    algos.append(("MiniBatchKMeans_k4", MiniBatchKMeans(n_clusters=4, random_state=42)))

    # 5-7 GMM 系列
    algos.append(("GMM_full_k2", GaussianMixture(n_components=2, covariance_type="full", random_state=42)))
    algos.append(("GMM_diag_k2", GaussianMixture(n_components=2, covariance_type="diag", random_state=42)))
    algos.append(("BayesianGMM_k2", BayesianGaussianMixture(n_components=2, random_state=42)))

    # 8-11 Agglomerative
    algos.append(("Agg_ward_k2", AgglomerativeClustering(n_clusters=2, linkage="ward")))
    algos.append(("Agg_average_k2", AgglomerativeClustering(n_clusters=2, linkage="average")))
    algos.append(("Agg_complete_k2", AgglomerativeClustering(n_clusters=2, linkage="complete")))
    algos.append(("Agg_single_k2", AgglomerativeClustering(n_clusters=2, linkage="single")))

    # 12 BIRCH
    algos.append(("BIRCH_k2", Birch(n_clusters=2)))

    # 13 Spectral
    algos.append(("Spectral_k2", SpectralClustering(n_clusters=2, assign_labels="kmeans", random_state=42)))

    # 14 DBSCAN
    algos.append(("DBSCAN", DBSCAN(eps=0.5, min_samples=5)))

    # 15 OPTICS
    algos.append(("OPTICS", OPTICS(min_samples=10, xi=0.05)))

    # 16 MeanShift
    algos.append(("MeanShift", MeanShift()))

    # 17 AffinityPropagation
    algos.append(("AffinityPropagation", AffinityPropagation(damping=0.9, random_state=42)))

    # 18 KMedoids
    if SKLEARN_EXTRA_OK:
        algos.append(("KMedoids_k2", KMedoids(n_clusters=2, random_state=42)))

    # HDBSCAN（視安裝情況）
    if HDBSCAN_OK:
        algos.append(("HDBSCAN", "HDBSCAN"))

    return algos


algorithms = make_algorithms()

print("實際執行演算法數量：", len(algorithms))


# -----------------------------
# 5. 評估函式
# -----------------------------
from sklearn.metrics import silhouette_score, precision_score, recall_score, f1_score
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score

def evaluate(y_true, labels):
    labels = np.asarray(labels)
    unique = set(labels)
    n_clusters = len(unique - {-1}) if len(unique) > 1 else 1

    # 評估三大叢集指標
    try:
        sil = silhouette_score(X_sample_pca, labels) if n_clusters > 1 else np.nan
    except:
        sil = np.nan

    try:
        ch = calinski_harabasz_score(X_sample_pca, labels) if n_clusters > 1 else np.nan
    except:
        ch = np.nan

    try:
        db = davies_bouldin_score(X_sample_pca, labels) if n_clusters > 1 else np.nan
    except:
        db = np.nan

    # 找詐欺比例最高 cluster
    df_tmp = pd.DataFrame({"y": y_true, "c": labels})

    if df_tmp["c"].nunique() <= 1:
        return n_clusters, sil, ch, db, None, np.nan, 0, 0, 0

    fraud_ratio = df_tmp.groupby("c")["y"].mean()
    fraud_cluster = fraud_ratio.idxmax()

    y_pred = (labels == fraud_cluster).astype(int)

    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    return n_clusters, sil, ch, db, fraud_cluster, fraud_ratio.max(), prec, rec, f1


# -----------------------------
# 6. 執行叢集
# -----------------------------
results = []

for name, algo in algorithms:
    print("=" * 60)
    print("執行：", name)

    try:
        if name == "HDBSCAN" and HDBSCAN_OK:
            model = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=10)
            labels = model.fit_predict(X_sample_pca)

        else:
            model = algo
            if hasattr(model, "fit_predict"):
                labels = model.fit_predict(X_sample_pca)
            else:
                model.fit(X_sample_pca)
                labels = model.labels_ if hasattr(model, "labels_") else model.predict(X_sample_pca)

        res = evaluate(y_sample, labels)

        results.append({
            "algorithm": name,
            "n_clusters": res[0],
            "silhouette": res[1],
            "calinski_harabasz": res[2],
            "davies_bouldin": res[3],
            "fraud_cluster": res[4],
            "fraud_ratio_in_cluster": res[5],
            "precision": res[6],
            "recall": res[7],
            "f1": res[8]
        })

        print("F1:", res[8])

    except Exception as e:
        print("錯誤：", e)
        results.append({
            "algorithm": name,
            "n_clusters": np.nan,
            "silhouette": np.nan,
            "calinski_harabasz": np.nan,
            "davies_bouldin": np.nan,
            "fraud_cluster": None,
            "fraud_ratio_in_cluster": np.nan,
            "precision": 0,
            "recall": 0,
            "f1": 0
        })


# -----------------------------
# 7. 輸出結果
# -----------------------------
results_df = pd.DataFrame(results)
results_df_sorted = results_df.sort_values("f1", ascending=False)

print("\n===== 最終排序（依 F1）=====")
print(results_df_sorted)

results_df_sorted.to_csv("clustering_creditcard_fraud_18_algorithms.csv", index=False)
print("\n已輸出 clustering_creditcard_fraud_18_algorithms.csv")

```
