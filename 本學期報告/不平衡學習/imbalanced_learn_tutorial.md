---
title: "imbalanced-learn æ•™å­¸æ–‡ä»¶"
author: "T Ben"
date: 2025-10-29
---

# imbalanced-learn æ•™å­¸æ–‡ä»¶ ï½œ Imbalanced-learn Tutorial
*(å« Matplotlib + Seaborn è¦–è¦ºåŒ–å¯¦ä½œèˆ‡é‡é»æ‘˜è¦)*

---

## ğŸ“˜ ä¸€ã€æ¨¡çµ„ä»‹ç´¹ ï½œ Introduction

`imbalanced-learn`ï¼ˆç°¡ç¨± `imblearn`ï¼‰æ˜¯ä¸€å€‹å°ˆé–€è§£æ±ºã€Œè³‡æ–™ä¸å¹³è¡¡ï¼ˆImbalanced Datasetï¼‰ã€å•é¡Œçš„ Python å¥—ä»¶ï¼Œå¸¸èˆ‡ `scikit-learn` ä¸€èµ·ä½¿ç”¨ã€‚

### âœ… Key Takeaways
- `imbalanced-learn` ä¸»è¦ç”¨æ–¼å¹³è¡¡é¡åˆ¥è³‡æ–™åˆ†ä½ˆã€‚
- å¯èˆ‡ sklearn pipeline ç„¡ç¸«æ•´åˆã€‚
- æä¾›å¤šç¨®éæ¡æ¨£èˆ‡æ¬ æ¡æ¨£æŠ€è¡“ã€‚

---

## âš–ï¸ äºŒã€è³‡æ–™ä¸å¹³è¡¡çš„å®šç¾© ï½œ What is Data Imbalance?

ç•¶æŸé¡åˆ¥æ¨£æœ¬çš„æ•¸é‡é å°‘æ–¼å…¶ä»–é¡åˆ¥ï¼ˆä¾‹å¦‚ 95% vs 5%ï¼‰æ™‚ï¼Œåˆ†é¡æ¨¡å‹å®¹æ˜“åå‘å¤šæ•¸é¡åˆ¥ï¼Œå°è‡´éŒ¯èª¤é æ¸¬ã€‚

| é¡å‹ | èªªæ˜ | ç¯„ä¾‹ |
|------|------|------|
| é¡åˆ¥æ¯”ä¾‹å¤±è¡¡ | æŸé¡æ¨£æœ¬éå°‘ | ä¿¡ç”¨å¡è©æ¬º (fraud: 1%, normal: 99%) |
| æ¨™ç±¤ç¨€æœ‰äº‹ä»¶ | ç½•è¦‹äº‹ä»¶é›£ä»¥åµæ¸¬ | å·¥æ¥­è¨­å‚™æ•…éšœé æ¸¬ |
| å¤šåˆ†é¡åæ…‹ | æŸå¹¾é¡æ¨£æœ¬ä½”æ“šå¤§å¤šæ•¸ | å¤šç–¾ç—…åˆ†é¡è³‡æ–™é›† |

### âœ… Key Takeaways
- ç•¶è³‡æ–™ä¸å¹³è¡¡æ™‚ï¼ŒAccuracy ä¸å†å¯é ã€‚
- æ¨¡å‹æœƒå‚¾å‘é æ¸¬å¤šæ•¸é¡åˆ¥ã€‚

---

## ğŸ§  ä¸‰ã€ä¸»è¦è§£æ±ºç­–ç•¥ ï½œ Common Strategies

1. **æ¬Šé‡èª¿æ•´ (Class Weighting)**  
2. **æ¬ æ¡æ¨£ (Under-sampling)**  
3. **éæ¡æ¨£ (Over-sampling)**  
4. **æ··åˆæ¡æ¨£ (Hybrid Sampling)**  

### âœ… Key Takeaways
- æ¬Šé‡èª¿æ•´é©åˆä¸­ç­‰ä¸å¹³è¡¡å•é¡Œã€‚
- SMOTE èˆ‡ ADASYN æ˜¯å¸¸è¦‹çš„éæ¡æ¨£æ–¹æ³•ã€‚

---

## ğŸ§° å››ã€å¸¸ç”¨æ–¹æ³• ï½œ Common imblearn Methods

| é¡åˆ¥ | æ–¹æ³• | èªªæ˜ |
|------|------|------|
| æ¬ æ¡æ¨£ | `RandomUnderSampler` | éš¨æ©Ÿåˆªé™¤å¤šæ•¸é¡æ¨£æœ¬ |
| éæ¡æ¨£ | `RandomOverSampler` | è¤‡è£½å°‘æ•¸é¡æ¨£æœ¬ |
| éæ¡æ¨£ | `SMOTE` | ä½¿ç”¨ KNN ç”Ÿæˆæ–°æ¨£æœ¬ |
| éæ¡æ¨£ | `ADASYN` | é‡å°é›£åˆ†é¡æ¨£æœ¬åˆæˆæ–°è³‡æ–™ |
| éæ¡æ¨£ | `KMeansSMOTE` | åˆ©ç”¨èšé¡è³‡è¨Šç”¢ç”Ÿæ›´å¹³è¡¡çš„æ–°æ¨£æœ¬ |
| æ··åˆ | `SMOTEENN`, `SMOTETomek` | çµåˆéæ¡æ¨£èˆ‡æ¸…ç†ç­–ç•¥ |

### âœ… Key Takeaways
- SMOTE æ˜¯æœ€å¸¸ç”¨çš„ç”Ÿæˆæ–¹æ³•ã€‚
- KMeansSMOTE å°é«˜ç¶­åº¦è³‡æ–™è¡¨ç¾è¼ƒç©©å®šã€‚

---

## ğŸ’» äº”ã€Python å¯¦ä½œ ï½œ Basic Implementation

### (1) å®‰è£å¥—ä»¶
```bash
pip install imbalanced-learn seaborn matplotlib scikit-learn
```

### (2) å»ºç«‹ä¸å¹³è¡¡è³‡æ–™é›†
```python
from sklearn.datasets import make_classification
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter

X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,
                           weights=[0.9, 0.1], random_state=42)
print(Counter(y))

sns.countplot(x=y)
plt.title("Before Resampling")
plt.show()
```

### âœ… Key Takeaways
- ä½¿ç”¨ `make_classification` å¿«é€Ÿå»ºç«‹æ¸¬è©¦è³‡æ–™é›†ã€‚
- å¯ä½¿ç”¨ Seaborn ç¹ªè£½é¡åˆ¥åˆ†ä½ˆåœ–ã€‚

---

## ğŸ”„ å…­ã€SMOTE éæ¡æ¨£ç¤ºä¾‹ ï½œ SMOTE Example
```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)
print(Counter(y_res))

sns.countplot(x=y_res)
plt.title("After SMOTE Resampling")
plt.show()
```

### âœ… Key Takeaways
- SMOTE å¯ç”Ÿæˆåˆæˆæ¨£æœ¬ï¼Œé¿å…å–®ç´”é‡è¤‡è³‡æ–™ã€‚
- æœ‰åŠ©æ–¼æå‡å°‘æ•¸é¡æ¨¡å‹è­˜åˆ¥ç‡ã€‚

---

## âš™ï¸ ä¸ƒã€ADASYN èˆ‡ KMeansSMOTE æ¯”è¼ƒ ï½œ ADASYN vs KMeansSMOTE

```python
from imblearn.over_sampling import ADASYN, KMeansSMOTE

ada = ADASYN(random_state=42)
X_ada, y_ada = ada.fit_resample(X, y)

kms = KMeansSMOTE(random_state=42)
X_kms, y_kms = kms.fit_resample(X, y)

fig, axs = plt.subplots(1, 3, figsize=(12, 4))
sns.countplot(x=y, ax=axs[0]); axs[0].set_title("Original")
sns.countplot(x=y_ada, ax=axs[1]); axs[1].set_title("ADASYN")
sns.countplot(x=y_kms, ax=axs[2]); axs[2].set_title("KMeansSMOTE")
plt.tight_layout()
plt.show()
```

### âœ… Key Takeaways
- ADASYN æ›´æ³¨é‡ç”Ÿæˆã€Œé›£åˆ†é¡ã€æ¨£æœ¬ã€‚
- KMeansSMOTE çµåˆèšé¡çµæœä»¥ç”¢ç”Ÿæ›´å¹³è¡¡çš„è³‡æ–™ã€‚

---

## ğŸ§© å…«ã€èˆ‡æ¨¡å‹æ•´åˆ ï½œ Model Integration (Pipeline)
```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

model = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('rf', RandomForestClassifier(random_state=42))
])

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

### âœ… Key Takeaways
- å¯å°‡ SMOTE èˆ‡æ¨¡å‹æ•´åˆé€²åŒä¸€ Pipelineã€‚
- é¿å…è³‡æ–™æ´©æ¼ï¼ˆData Leakageï¼‰ã€‚

---

## ğŸ“Š ä¹ã€æ€§èƒ½æ¯”è¼ƒåœ– ï½œ Performance Comparison
```python
from sklearn.metrics import f1_score, roc_auc_score
import numpy as np

methods = {
    'SMOTE': SMOTE(random_state=42),
    'ADASYN': ADASYN(random_state=42),
    'KMeansSMOTE': KMeansSMOTE(random_state=42)
}

scores = {}
for name, sampler in methods.items():
    X_res, y_res = sampler.fit_resample(X_train, y_train)
    clf = RandomForestClassifier(random_state=42)
    clf.fit(X_res, y_res)
    y_pred = clf.predict(X_test)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])
    scores[name] = (f1, auc)

f1s = [v[0] for v in scores.values()]
aucs = [v[1] for v in scores.values()]

fig, ax = plt.subplots(1,2, figsize=(10,4))
sns.barplot(x=list(scores.keys()), y=f1s, ax=ax[0])
ax[0].set_title("F1 Score Comparison")

sns.barplot(x=list(scores.keys()), y=aucs, ax=ax[1])
ax[1].set_title("ROC-AUC Comparison")

plt.tight_layout()
plt.show()
```

### âœ… Key Takeaways
- ä½¿ç”¨ F1-score èˆ‡ ROC-AUC è¡¡é‡å°‘æ•¸é¡åˆ¥è¡¨ç¾ã€‚
- ä¸åŒæ–¹æ³•é©åˆä¸åŒè³‡æ–™å‹æ…‹ã€‚

---

## âš™ï¸ åã€å»¶ä¼¸ç« ç¯€ï¼šXGBoost / LightGBM èª¿åƒæŠ€å·§ ï½œ Advanced Tuning

### XGBoost ç¯„ä¾‹
```python
import xgboost as xgb
clf = xgb.XGBClassifier(
    scale_pos_weight=9,
    eval_metric='aucpr',
    learning_rate=0.1,
    max_depth=5,
    n_estimators=200,
    random_state=42
)
clf.fit(X_train, y_train)
print(classification_report(y_test, clf.predict(X_test)))
```

### LightGBM ç¯„ä¾‹
```python
import lightgbm as lgb
clf = lgb.LGBMClassifier(
    scale_pos_weight=9,
    objective='binary',
    metric='auc',
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=200,
    random_state=42
)
clf.fit(X_train, y_train)
print(classification_report(y_test, clf.predict(X_test)))
```

### âœ… Key Takeaways
- XGBoost / LightGBM å¯ä½¿ç”¨ `scale_pos_weight` èª¿æ•´æ¨£æœ¬æ¬Šé‡ã€‚
- é¸ç”¨ `aucpr` è©•ä¼°æŒ‡æ¨™é©åˆä¸å¹³è¡¡æƒ…å¢ƒã€‚

---

## âœ… åä¸€ã€ç¸½çµ ï½œ Summary
- `imbalanced-learn` æä¾›å¤šæ¨£åŒ–çš„è³‡æ–™å¹³è¡¡æ–¹æ³•ã€‚
- å¯è¼•é¬†æ•´åˆè‡³ sklearn pipelineã€‚
- å¯çµåˆ XGBoost / LightGBM é€²è¡Œé€²éšæ‡‰ç”¨ã€‚

---

## ğŸ“š åƒè€ƒè³‡æº ï½œ References
- [imbalanced-learn å®˜æ–¹æ–‡ä»¶](https://imbalanced-learn.org/stable/)
- [scikit-learn å®˜æ–¹æ–‡ä»¶](https://scikit-learn.org/stable/)
- [XGBoost å®˜æ–¹æ–‡ä»¶](https://xgboost.readthedocs.io/)
- [LightGBM å®˜æ–¹æ–‡ä»¶](https://lightgbm.readthedocs.io/)

---

