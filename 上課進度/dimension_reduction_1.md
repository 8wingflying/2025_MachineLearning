## ğŸ§  é™ç¶­ï¼ˆDimension Reductionï¼‰æ•™å­¸æ–‡ä»¶

### ğŸ“˜ ç¬¬ 1 ç« ï¼šé™ç¶­çš„æ¦‚å¿µèˆ‡ç›®çš„

**é™ç¶­ï¼ˆDimension Reductionï¼‰** æ˜¯è³‡æ–™å‰è™•ç†ä¸­æ¥µç‚ºé‡è¦çš„ä¸€æ­¥ï¼Œä¸»è¦ç›®çš„åœ¨æ–¼ï¼š
- å»é™¤å†—é¤˜ç‰¹å¾µã€é™ä½å™ªéŸ³ã€‚
- æ¸›å°‘æ¨¡å‹è¨“ç·´æ™‚é–“èˆ‡å„²å­˜æˆæœ¬ã€‚
- ä¿ç•™è³‡æ–™çš„ä¸»è¦çµæ§‹èˆ‡è³‡è¨Šã€‚
- æœ‰åŠ©æ–¼è³‡æ–™è¦–è¦ºåŒ–ï¼ˆ2D / 3D å±•ç¤ºé«˜ç¶­è³‡æ–™ï¼‰ã€‚

å¸¸è¦‹æ‡‰ç”¨ï¼š
- å½±åƒç‰¹å¾µå£“ç¸®ï¼ˆå¦‚ CNN feature embeddingï¼‰
- NLP è©åµŒå…¥å¾Œçš„å¯è¦–åŒ–ï¼ˆå¦‚ Word2Vec + t-SNEï¼‰
- æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ç‰¹å¾µé¸æ“‡å‰çš„å‰è™•ç†

---

### ğŸ“Š ç¬¬ 2 ç« ï¼šé™ç¶­æ–¹æ³•åˆ†é¡

| æ–¹æ³•é¡åˆ¥ | åç¨± | ç‰¹æ€§ |
|-----------|------|------|
| **ç·šæ€§æ–¹æ³•** | PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰ | ä¿ç•™æœ€å¤§æ–¹å·®æ–¹å‘ï¼Œå‡è¨­ç·šæ€§é—œä¿‚ |
| | LDAï¼ˆç·šæ€§åˆ¤åˆ¥åˆ†æï¼‰ | ç›£ç£å¼é™ç¶­ï¼Œæœ€å¤§åŒ–é¡åˆ¥é–“è·é›¢ |
| **éç·šæ€§æ–¹æ³•** | t-SNE | ä¿ç•™é„°è¿‘é—œä¿‚ï¼Œé©åˆé«˜ç¶­è³‡æ–™è¦–è¦ºåŒ– |
| | UMAP | é«˜é€Ÿä¸”ä¿æŒå±€éƒ¨èˆ‡å…¨åŸŸçµæ§‹ |
| | AutoEncoder | ä½¿ç”¨ç¥ç¶“ç¶²è·¯è‡ªå‹•å­¸ç¿’ä½ç¶­ç‰¹å¾µè¡¨ç¤º |

---

### ğŸ§© ç¬¬ 3 ç« ï¼šPCA ä¸»æˆåˆ†åˆ†æ

**PCAï¼ˆPrincipal Component Analysisï¼‰** é€éç‰¹å¾µå€¼åˆ†è§£å”æ–¹å·®çŸ©é™£ï¼Œæ‰¾å‡ºè³‡æ–™æ–¹å·®æœ€å¤§çš„æ–¹å‘ã€‚

#### ğŸ§® æ•¸å­¸åŸç†
1. æ¨™æº–åŒ–è³‡æ–™ã€‚
2. è¨ˆç®—å”æ–¹å·®çŸ©é™£ \( \Sigma = \frac{1}{n-1} X^T X \)
3. ç‰¹å¾µå€¼åˆ†è§£ \( \Sigma v_i = \lambda_i v_i \)
4. é¸å–æœ€å¤§ \( k \) å€‹ç‰¹å¾µå‘é‡çµ„æˆæŠ•å½±çŸ©é™£ã€‚

#### ğŸ§‘â€ğŸ’» Python ç¯„ä¾‹
```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

X = load_iris().data
y = load_iris().target

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.title('PCA on Iris Dataset')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()
```

---

### ğŸ§­ ç¬¬ 4 ç« ï¼šLDA ç·šæ€§åˆ¤åˆ¥åˆ†æ

**LDAï¼ˆLinear Discriminant Analysisï¼‰** æ˜¯ç›£ç£å¼é™ç¶­ï¼Œé€éæœ€å¤§åŒ–é¡é–“æ•£ä½ˆèˆ‡æœ€å°åŒ–é¡å…§æ•£ä½ˆé”æˆåˆ†é¡æ•ˆæœæœ€ä½³çš„æŠ•å½±ã€‚

#### ğŸ§® ç›®æ¨™å‡½æ•¸
\[
\max_W \frac{|W^T S_B W|}{|W^T S_W W|}
\]

#### ğŸ§‘â€ğŸ’» Python ç¯„ä¾‹
```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

X = load_iris().data
y = load_iris().target

lda = LDA(n_components=2)
X_lda = lda.fit_transform(X, y)

plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='rainbow')
plt.title('LDA on Iris Dataset')
plt.xlabel('LD1')
plt.ylabel('LD2')
plt.show()
```

---

### ğŸŒˆ ç¬¬ 5 ç« ï¼št-SNEï¼ˆt-Distributed Stochastic Neighbor Embeddingï¼‰

**t-SNE** æ˜¯ä¸€ç¨®éç·šæ€§é™ç¶­æ–¹æ³•ï¼Œé©åˆé«˜ç¶­è³‡æ–™çš„è¦–è¦ºåŒ–ã€‚å®ƒæœƒï¼š
- å°‡é«˜ç¶­ç©ºé–“çš„ç›¸ä¼¼åº¦æ˜ å°„åˆ°ä½ç¶­ç©ºé–“ã€‚
- ä¿ç•™é„°è¿‘é»çš„ç›¸å°é—œä¿‚ï¼ˆå±€éƒ¨çµæ§‹ï¼‰ã€‚

#### ğŸ§‘â€ğŸ’» Python ç¯„ä¾‹
```python
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

X, y = load_digits(return_X_y=True)
X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)

plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10')
plt.title('t-SNE on Digits Dataset')
plt.show()
```

---

### ğŸš€ ç¬¬ 6 ç« ï¼šUMAPï¼ˆUniform Manifold Approximation and Projectionï¼‰

**UMAP** æ˜¯è¿‘å¹´éå¸¸æµè¡Œçš„éç·šæ€§é™ç¶­æ–¹æ³•ï¼Œèƒ½ä¿ç•™å±€éƒ¨èˆ‡å…¨åŸŸçµæ§‹ï¼Œä¸”é€Ÿåº¦é å¿«æ–¼ t-SNEã€‚

#### ğŸ§‘â€ğŸ’» Python ç¯„ä¾‹
```python
import umap
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

X, y = load_digits(return_X_y=True)
reducer = umap.UMAP(n_neighbors=15, min_dist=0.1)
X_umap = reducer.fit_transform(X)

plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='Spectral')
plt.title('UMAP on Digits Dataset')
plt.show()
```

---

### ğŸ¤– ç¬¬ 7 ç« ï¼šAutoEncoder éç·šæ€§é™ç¶­ï¼ˆæ·±åº¦å­¸ç¿’æ–¹æ³•ï¼‰

**AutoEncoder** æ˜¯ä¸€ç¨®ç„¡ç›£ç£å¼ç¥ç¶“ç¶²è·¯ï¼Œç”¨æ–¼å­¸ç¿’è¼¸å…¥è³‡æ–™çš„å£“ç¸®è¡¨ç¤ºã€‚å®ƒåŒ…å«ï¼š
- **Encoder**ï¼šå°‡é«˜ç¶­è³‡æ–™å£“ç¸®æˆä½ç¶­æ½›åœ¨å‘é‡ï¼ˆlatent vectorï¼‰ã€‚
- **Decoder**ï¼šå¾ä½ç¶­æ½›åœ¨ç©ºé–“é‡å»ºåŸå§‹è³‡æ–™ã€‚

#### ğŸ§  åŸç†ç¤ºæ„
```
Input â†’ [Encoder] â†’ Latent Representation â†’ [Decoder] â†’ Reconstructed Output
```

#### ğŸ§‘â€ğŸ’» Python ç¯„ä¾‹ï¼ˆKerasï¼‰
```python
import numpy as np
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# è¼‰å…¥è³‡æ–™
X = load_iris().data

# å®šç¾© AutoEncoder çµæ§‹
input_dim = X.shape[1]
encoding_dim = 2  # å£“ç¸®åˆ° 2 ç¶­

input_layer = Input(shape=(input_dim,))
encoded = Dense(4, activation='relu')(input_layer)
encoded = Dense(encoding_dim, activation='linear')(encoded)
decoded = Dense(4, activation='relu')(encoded)
decoded = Dense(input_dim, activation='linear')(decoded)

autoencoder = Model(inputs=input_layer, outputs=decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# è¨“ç·´æ¨¡å‹
autoencoder.fit(X, X, epochs=200, batch_size=16, verbose=0)

# å–å‡º Encoder éƒ¨åˆ†
encoder = Model(inputs=input_layer, outputs=encoded)
X_encoded = encoder.predict(X)

# è¦–è¦ºåŒ–ä½ç¶­è¡¨ç¤º
plt.scatter(X_encoded[:, 0], X_encoded[:, 1], c=load_iris().target, cmap='viridis')
plt.title('AutoEncoder Nonlinear Dimension Reduction')
plt.xlabel('Latent Dim 1')
plt.ylabel('Latent Dim 2')
plt.show()
```

#### âœ… å„ªé»
- å¯å­¸ç¿’éç·šæ€§çµæ§‹ã€‚
- å¯æ“´å±•è‡³æ·±å±¤ç¶²è·¯ï¼ˆDeep AutoEncoderï¼‰ã€‚

#### âš ï¸ ç¼ºé»
- è¨“ç·´æˆæœ¬é«˜ã€éœ€è¦å¤§é‡è³‡æ–™ã€‚
- çµæœä¾è³´ç¶²è·¯è¨­è¨ˆèˆ‡åˆå§‹åŒ–ã€‚

---

### ğŸ§  ç¬¬ 8 ç« ï¼šæ–¹æ³•æ¯”è¼ƒ

| æ–¹æ³• | å±¬æ€§ | ä¿ç•™å±€éƒ¨çµæ§‹ | ä¿ç•™å…¨åŸŸçµæ§‹ | è¨ˆç®—é€Ÿåº¦ | é©ç”¨å ´æ™¯ |
|------|------|--------------|--------------|----------|----------|
| PCA  | ç·šæ€§ | âŒ | âœ… | ğŸš€ å¿« | ç·šæ€§ç‰¹å¾µåˆ†æ |
| LDA  | ç·šæ€§ï¼ˆç›£ç£ï¼‰ | âœ… | âœ… | ğŸš€ å¿« | é¡åˆ¥å€åˆ† |
| t-SNE | éç·šæ€§ | âœ… | âŒ | ğŸ¢ æ…¢ | é«˜ç¶­è³‡æ–™è¦–è¦ºåŒ– |
| UMAP | éç·šæ€§ | âœ… | âœ… | âš¡ å¿« | é«˜ç¶­åµŒå…¥èˆ‡èšé¡ |
| AutoEncoder | éç·šæ€§ | âœ… | âœ… | âš™ï¸ ä¸­ | æ·±åº¦å­¸ç¿’å£“ç¸®èˆ‡ç‰¹å¾µèƒå– |

---

### ğŸ“˜ ç¬¬ 9 ç« ï¼šå»¶ä¼¸ç·´ç¿’å»ºè­°

1. æ¯”è¼ƒ PCAã€t-SNEã€UMAPã€AutoEncoder åœ¨ `MNIST` è³‡æ–™ä¸Šçš„é™ç¶­çµæœã€‚
2. æ¢ç´¢ **Variational AutoEncoder (VAE)** ä½œç‚ºæ©Ÿç‡å¼é™ç¶­ã€‚
3. å°‡é™ç¶­å¾Œè³‡æ–™è¼¸å…¥ SVMã€RandomForest ç­‰åˆ†é¡å™¨ï¼Œæ¯”è¼ƒæº–ç¢ºç‡å·®ç•°ã€‚
4. å°é«˜ç¶­åµŒå…¥å‘é‡ï¼ˆä¾‹å¦‚ BERTï¼‰ä½¿ç”¨ UMAP / AutoEncoder é€²è¡Œå¯è¦–åŒ–ã€‚

---

### ğŸ¯ ç¬¬ 10 ç« ï¼šç¸½çµ

- é™ç¶­æ˜¯æ©Ÿå™¨å­¸ç¿’èˆ‡è³‡æ–™è¦–è¦ºåŒ–çš„é‡è¦æ­¥é©Ÿã€‚
- ç·šæ€§æ–¹æ³•ï¼ˆPCAã€LDAï¼‰ç°¡å–®å¿«é€Ÿï¼Œéç·šæ€§æ–¹æ³•ï¼ˆt-SNEã€UMAPã€AutoEncoderï¼‰èƒ½æ•æ‰æ›´è¤‡é›œçš„çµæ§‹ã€‚
- AutoEncoder å°‡å‚³çµ±çµ±è¨ˆèˆ‡æ·±åº¦å­¸ç¿’çµåˆï¼Œæˆç‚ºç¾ä»£é™ç¶­çš„é‡è¦å·¥å…·ã€‚

