# Bernoulli Naive Bayes Tutorial ä¼¯åŠªåˆ©è´æ°åˆ†é¡å™¨æ•™å­¸

---

## Part 1 | Concept & Intuition æ¦‚å¿µèˆ‡ç›´è¦º

**Bernoulli Naive Bayes (BNB)** é©ç”¨æ–¼ **äºŒå…ƒç‰¹å¾µè³‡æ–™**ï¼ˆä¾‹å¦‚æ–‡å­—æ˜¯å¦å‡ºç¾ã€äº‹ä»¶æ˜¯å¦ç™¼ç”Ÿã€0/1 ç‰¹å¾µï¼‰ã€‚
å®ƒå‡è¨­æ¯å€‹ç‰¹å¾µéƒ½éµå¾ª **ä¼¯åŠªåˆ©åˆ†ä½ˆ (Bernoulli Distribution)**ï¼Œå³åªæœ‰ã€Œå‡ºç¾ (1)ã€æˆ–ã€Œæœªå‡ºç¾ (0)ã€å…©ç¨®æƒ…æ³ã€‚

### âœ… é©ç”¨æƒ…å¢ƒ
- æ–‡å­—åˆ†é¡ï¼ˆä¾‹å¦‚åƒåœ¾éƒµä»¶éæ¿¾ï¼‰  
- ç‰¹å¾µæ˜¯å¸ƒæ—å€¼ï¼ˆä¾‹å¦‚æ˜¯å¦é»æ“Šã€æ˜¯å¦é–‹å•Ÿéƒµä»¶ï¼‰  
- Binary encoding ç‰¹å¾µ

---

## Part 2 | Mathematical Foundation æ•¸å­¸åŸºç¤

å°æ–¼æ¯ä¸€é¡åˆ¥ \( C_k \)ï¼ŒBernoulli Naive Bayes çš„æ¢ä»¶æ©Ÿç‡ç‚ºï¼š

\[
P(X|C_k) = \prod_{i=1}^{n} P(x_i|C_k)^{x_i} \cdot (1 - P(x_i|C_k))^{(1 - x_i)}
\]

åˆ†é¡è¦å‰‡ï¼š
\[
\hat{C} = \arg\max_{C_k} P(C_k) \prod_{i=1}^{n} P(x_i|C_k)^{x_i} (1 - P(x_i|C_k))^{1 - x_i}
\]

---

## Part 3 | Python Implementation (åƒåœ¾éƒµä»¶ Spam vs Ham)

```python
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Sample dataset
texts = [
    "Win a free iPhone now", "Lowest price guaranteed",
    "Hey, are we meeting tomorrow?", "Your invoice is attached",
    "Congratulations, you won lottery", "Let's grab lunch today"
]
labels = [1, 1, 0, 0, 1, 0]  # 1 = spam, 0 = ham

# Binary bag-of-words representation
vectorizer = CountVectorizer(binary=True)
X = vectorizer.fit_transform(texts)
y = labels

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train BernoulliNB model
bnb = BernoulliNB()
bnb.fit(X_train, y_train)

# Predict
y_pred = bnb.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
```

---

## Part 4 | Feature Log Probabilities ç‰¹å¾µæ¼”åŒ–æ¯”é‡

```python
import numpy as np

feature_names = np.array(vectorizer.get_feature_names_out())
spam_class_index = list(bnb.classes_).index(1)
top_features = np.argsort(bnb.feature_log_prob_[spam_class_index])[-10:]

print("Spam keywords:")
print(feature_names[top_features])
```

---

## Part 5 | Comparison with Other Naive Bayes Models æ¯”è¼ƒè¡¨

| æ¨¡å‹é¡å‹ | ç‰¹å¾µå‹æ…‹ | åˆ†ä½ˆå‡è¨­ | å¸¸è¦‹æ‡‰ç”¨ |
|-----------|-----------|-----------|-----------|
| **GaussianNB** | é€£çºŒ | æ­£æ…‹åˆ†ä½ˆ | æ•¸å€¼å‹ç‰¹å¾µ |
| **MultinomialNB** | è¨ˆæ•¸ | å¤šé …åˆ†ä½ˆ | æ–‡å­—åˆ†é¡ï¼ˆå­—é »ï¼‰ |
| **BernoulliNB** | äºŒå…ƒ | ä¼¯åŠªåˆ©åˆ†ä½ˆ | æ–‡å­—æ˜¯å¦å‡ºç¾ |

---

## Part 6 | Performance Comparison: BernoulliNB vs MultinomialNB

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score

# Prepare vectorizer for MultinomialNB (use frequency counts)
vectorizer_count = CountVectorizer(binary=False)
X_count = vectorizer_count.fit_transform(texts)

bnb_scores = cross_val_score(BernoulliNB(), X, y, cv=3)
mnb_scores = cross_val_score(MultinomialNB(), X_count, y, cv=3)

print(f"BernoulliNB Accuracy: {bnb_scores.mean():.3f}")
print(f"MultinomialNB Accuracy: {mnb_scores.mean():.3f}")
```

**åˆ†æçµæœï¼š**
- è‹¥æ–‡æœ¬çŸ­ã€è©é »è³‡è¨Šä¸è¶³ â†’ BernoulliNB è¡¨ç¾è¼ƒå¥½ã€‚
- è‹¥æ–‡æœ¬é•·ä¸”å­—è©é »ç‡å·®ç•°å¤§ â†’ MultinomialNB é€šå¸¸æ›´æº–ç¢ºã€‚

---

## Part 7 | Extended Chapter: TF-IDF + BernoulliNB æ–‡å­—åˆ†é¡å¯¦æˆ° Ã— å¯è¦–åŒ–çµæœ

### ğŸ“˜ å¯¦æˆ°ç›®æ¨™
ä½¿ç”¨ **TF-IDF å‘é‡åŒ–** çµåˆ BernoulliNB é€²è¡Œæ–‡å­—åˆ†é¡ï¼Œä¸¦å¯è¦–åŒ–åˆ†é¡çµæœçš„ä¿¡å¿ƒåˆ†å¸ƒã€‚

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# ç¯„ä¾‹è³‡æ–™é›†
texts = [
    "Win money now!", "Meeting schedule update",
    "Free vacation offer", "Important project deadline",
    "Congratulations, you are selected", "Let's have lunch tomorrow"
]
labels = [1, 0, 1, 0, 1, 0]  # 1 = spam, 0 = ham

# TF-IDF å‘é‡åŒ–
vectorizer_tfidf = TfidfVectorizer(stop_words='english', binary=True)
X_tfidf = vectorizer_tfidf.fit_transform(texts)

# æ¨¡å‹è¨“ç·´
bnb_tfidf = BernoulliNB()
bnb_tfidf.fit(X_tfidf, labels)

# é æ¸¬èˆ‡ä¿¡å¿ƒåˆ†æ•¸
probs = bnb_tfidf.predict_proba(X_tfidf)[:, 1]

# PCA è¦–è¦ºåŒ–
pca = PCA(n_components=2).fit_transform(X_tfidf.toarray())
plt.figure(figsize=(6, 4))
plt.scatter(pca[:, 0], pca[:, 1], c=probs, cmap='coolwarm', s=100, edgecolors='k')
plt.colorbar(label='Spam Probability')
plt.title('TF-IDF + BernoulliNB Spam Classification Visualization')
plt.xlabel('PCA Dimension 1')
plt.ylabel('PCA Dimension 2')
plt.show()
```

### ğŸ“Š çµæœè§£è®€
- é¡è‰²è¶Šç´…ä»£è¡¨æ¨¡å‹è¶Šèªç‚ºè©²æ–‡æœ¬å±¬æ–¼ã€ŒSpamã€ã€‚  
- å¯è§€å¯Ÿåˆ†é¡é‚Šç•Œèˆ‡æ¨¡å‹ä¿¡å¿ƒåˆ†å¸ƒã€‚  
- è‹¥æ”¹ç”¨ MultinomialNBï¼Œæ•´é«”ä¿¡å¿ƒåˆ†ä½ˆæœƒæ›´å—è©é »å¼·åº¦å½±éŸ¿ã€‚

---

## Part 8 | Pros & Cons å„ªç¼ºé»åˆ†æ

| å„ªé» | ç¼ºé» |
|------|------|
| é©åˆå¸ƒæ—ç‰¹å¾µèˆ‡ç¨€ç–çŸ©é™£ | éäºŒå…ƒç‰¹å¾µéœ€è½‰æ› |
| è¨ˆç®—å¿«é€Ÿã€è¨“ç·´æ™‚é–“çŸ­ | å‡è¨­ç‰¹å¾µç¨ç«‹æ€§éæ–° |
| çŸ­æ–‡æœ¬åˆ†é¡æ•ˆæœè‰¯å¥½ | ç„¡æ³•åˆ©ç”¨å­—é »è³‡è¨Š |

---

## Part 9 | Practical Tips å¯¦å‹™å»ºè­°

1. å° Binary ç‰¹å¾µæœ€é©ç”¨ï¼Œå¦‚æ˜¯å¦å‡ºç¾ã€æ˜¯å¦é»æ“Šã€æ˜¯å¦å­˜åœ¨é—œéµè©ã€‚  
2. åœ¨çŸ­æ–‡æœ¬ä¸Šæ¯” MultinomialNB æ›´æœ‰æ•ˆï¼Œè€Œåœ¨é•·æ–‡æœ¬å‰‡ç›¸åã€‚  
3. çµ„åˆ TF-IDF å‘é‡åŒ–æˆ–ç‰¹å¾µé¸æ“‡ï¼Œå¯é¡¯è‘—æå‡æ•ˆèƒ½ã€‚  
4. å¯ä½œç‚º NLP baseline ç”¨ä¾†æ¸¬è©¦åŸºç¤æ¨¡å‹æ•ˆèƒ½ã€‚

---

âœ¨ **End of Tutorial** â€” `Bernoulli_NB_Tutorial.md`

