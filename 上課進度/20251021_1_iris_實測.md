# 🌸 使用 10 種監督式與 4 種非監督式機器學習演算法分析 Iris 資料集

---

## 📘 一、資料集介紹

- **資料來源**：`sklearn.datasets.load_iris()`
- **特徵 (features)**：
  - 花萼長度（sepal length）
  - 花萼寬度（sepal width）
  - 花瓣長度（petal length）
  - 花瓣寬度（petal width）
- **標籤 (labels)**：
  - 0 = Iris setosa  
  - 1 = Iris versicolor  
  - 2 = Iris virginica

---

## 📦 二、載入與分割資料

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

## 監督式
```
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier

models = {
    "LogisticRegression": LogisticRegression(max_iter=200),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "DecisionTree": DecisionTreeClassifier(),
    "RandomForest": RandomForestClassifier(),
    "GradientBoosting": GradientBoostingClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "NaiveBayes": GaussianNB(),
    "ExtraTrees": ExtraTreesClassifier(),
    "MLPClassifier": MLPClassifier(max_iter=500)
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"{name:20s} 準確率: {accuracy_score(y_test, y_pred):.4f}")

```

## 非監督式
```
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# PCA 降維至 2 維方便視覺化
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

algorithms = {
    "KMeans": KMeans(n_clusters=3, random_state=42),
    "DBSCAN": DBSCAN(eps=0.5, min_samples=5),
    "Agglomerative": AgglomerativeClustering(n_clusters=3)
}

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for ax, (name, algo) in zip(axes, algorithms.items()):
    labels = algo.fit_predict(X_pca)
    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='rainbow')
    ax.set_title(name)
plt.show()
```
