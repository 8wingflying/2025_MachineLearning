##
# GMM 教學文件（以 Python 物件導向撰寫）

## 1. 什麼是 GMM？

高斯混合模型（Gaussian Mixture Model, GMM）是一種「用多個高斯分佈相加」來描述資料的機率模型。

我們假設資料是由$ K$ 個高斯分佈（群）混合而成，每一個群各自有：
- 平均值（均值向量） $\mu_k$
- 共變異數矩陣 \(\Sigma_k\)
- 混合權重（該群出現的機率） \(\pi_k\)，且 \(\sum_k \pi_k = 1\)

整體觀察到一筆資料點 \(x\) 的機率密度是：

$$
p(x) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(x \mid \mu_k, \Sigma_k)
$$

這是「軟分群」模型，比 K-Means 更細緻：
- K-Means：一點只屬於一個群（硬指派）
- GMM：一點以機率方式同時屬於每個群（軟指派）


---

## 2. 我們要解決什麼問題？

給定一堆資料 \(\{x_1, x_2, ..., x_N\}\)，我們想學到：
- \(\pi_k\)：每個群的權重
- \(\mu_k\)：每個群的中心
- \(\Sigma_k\)：每個群的形狀/擴散程度（可是全矩陣、對角矩陣、同方差等假設）

這其實是「最大概似估計」問題（Maximum Likelihood Estimation, MLE）。

但直接解很難，因為我們不知道每點屬於哪個群。  
→ 傳統解法：使用 EM 演算法（Expectation-Maximization）反覆近似最佳參數。


---

## 3. EM 演算法（GMM 的心臟）

EM 每一步分兩個階段：

### (1) E-step（期望步驟）
根據目前的參數，計算每個資料點屬於第 \(k\) 個成分的機率，稱為「責任度」(responsibility)：

$$
\gamma_{nk} = r_{nk} = 
\frac{\pi_k \, \mathcal{N}(x_n \mid \mu_k, \Sigma_k)}
{\sum_{j=1}^{K} \pi_j \, \mathcal{N}(x_n \mid \mu_j, \Sigma_j)}
$$

直覺：  
- 如果某高斯剛好包住這個點，那它對這個點就「負更多責任」。

### (2) M-step（最大化步驟）
用上面算到的責任度，重新估計參數：

$$
N_k = \sum_{n=1}^{N} \gamma_{nk}
$$

更新混合權重：

$$
\pi_k = \frac{N_k}{N}
$$

更新均值：

$$
\mu_k = \frac{1}{N_k} \sum_{n=1}^{N} \gamma_{nk} x_n
$$

更新共變異數：

$$
\Sigma_k = \frac{1}{N_k} \sum_{n=1}^{N} \gamma_{nk} (x_n - \mu_k)(x_n - \mu_k)^\top
$$

然後重複 E-step 和 M-step，直到收斂（例如參數變動很小，或 log-likelihood 幾乎不再上升）。


---

## 4. 我們的 Python OOP 設計

我們將實作一個 `GaussianMixture` 類別，類似 `sklearn.mixture.GaussianMixture` 的感覺，但用最基本的 `numpy` 寫出來，幫助你理解細節。

### 4.1 類別設計概念

```text
GaussianMixture
 ├─ 屬性 (attributes)
 │   ├─ n_components      ：群數 K
 │   ├─ weights_          ：π_k，大きさ (K,)
 │   ├─ means_            ：μ_k，shape (K, D)
 │   ├─ covariances_      ：Σ_k，shape (K, D, D) 假設 full covariance
 │   ├─ log_likelihood_   ：訓練過程中最新的 log likelihood
 │
 ├─ 方法 (methods)
 │   ├─ __init__()        ：初始化超參數
 │   ├─ _initialize_params(X) ：用隨機/簡單策略初始化 π, μ, Σ
 │   ├─ _multivariate_gaussian(x, mean, cov) ：計算多維常態密度
 │   ├─ _e_step(X)        ：計算 γ_{nk}
 │   ├─ _m_step(X, resp)  ：更新 π, μ, Σ
 │   ├─ _compute_log_likelihood(X) ：整體對數概似
 │   ├─ fit(X)            ：執行 EM
 │   ├─ predict_proba(X)  ：回傳每個點屬於每個群的機率 γ_{nk}
 │   ├─ predict(X)        ：回傳最可能的群 index (argmax γ_{nk})
```

### 檔案 2：`gmm_oop.py`

```python
import numpy as np

class GaussianMixture:
    def __init__(
        self,
        n_components=2,
        max_iter=100,
        tol=1e-4,
        reg_covar=1e-6,
        random_state=None,
    ):
        """
        參數:
        - n_components: 群數 K
        - max_iter: EM 最多迭代次數
        - tol: 收斂門檻 (log-likelihood 改變幅度小於 tol 就停止)
        - reg_covar: 共變異數正則項，避免共變異數不可逆
        - random_state: 隨機種子 (int 或 None)
        """
        self.n_components = n_components
        self.max_iter = max_iter
        self.tol = tol
        self.reg_covar = reg_covar

        self.random_state = np.random.RandomState(random_state)

        # fit() 後才會被填入
        self.weights_ = None        # shape: (K,)
        self.means_ = None          # shape: (K, D)
        self.covariances_ = None    # shape: (K, D, D)

        self.log_likelihood_ = None # 最後一次迭代的 log-likelihood
        self.converged_ = False

    def _initialize_params(self, X):
        """
        初始化 π, μ, Σ
        """
        N, D = X.shape

        # 1. means_：從資料中隨機挑 K 筆
        indices = self.random_state.choice(N, self.n_components, replace=False)
        self.means_ = X[indices]

        # 2. covariances_：用整體資料共變異數，複製給每個群
        overall_cov = np.cov(X, rowvar=False)  # shape (D, D)
        self.covariances_ = np.array(
            [overall_cov + self.reg_covar * np.eye(D)
             for _ in range(self.n_components)]
        )

        # 3. weights_：平均分配
        self.weights_ = np.ones(self.n_components) / self.n_components

    def _multivariate_gaussian(self, X, mean, cov):
        """
        多變量常態分佈 N(x | mean, cov)
        X: shape (N, D)
        mean: shape (D,)
        cov: shape (D, D)
        return: shape (N,)
        """
        D = X.shape[1]
        cov_det = np.linalg.det(cov)
        if cov_det <= 0:
            cov_det = 1e-12
        cov_inv = np.linalg.inv(cov)

        norm_const = np.sqrt((2 * np.pi) ** D * cov_det)

        diff = X - mean  # (N, D)
        exp_term = np.einsum('ni,ij,nj->n', diff, cov_inv, diff)

        return np.exp(-0.5 * exp_term) / norm_const

    def _e_step(self, X):
        """
        E-step:
        計算 responsibilities γ_{nk}
        回傳 resp: shape (N, K)
        """
        N, _ = X.shape
        K = self.n_components

        resp = np.zeros((N, K))

        for k in range(K):
            resp[:, k] = self.weights_[k] * self._multivariate_gaussian(
                X,
                self.means_[k],
                self.covariances_[k],
            )

        # 正規化到每列和為1
        row_sums = resp.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1e-12
        resp /= row_sums

        return resp

    def _m_step(self, X, resp):
        """
        M-step:
        用 resp 更新 π, μ, Σ
        """
        N, D = X.shape
        K = self.n_components

        # N_k = sum_n γ_{nk}
        N_k = resp.sum(axis=0)  # (K,)

        # 更新 π_k
        self.weights_ = N_k / N  # (K,)

        # 更新 μ_k
        self.means_ = np.dot(resp.T, X) / N_k[:, np.newaxis]  # (K, D)

        # 更新 Σ_k
        new_covs = np.zeros((K, D, D))

        for k in range(K):
            diff = X - self.means_[k]  # (N, D)
            weighted_diff = diff * resp[:, k][:, np.newaxis]  # (N, D)
            cov_k = np.dot(weighted_diff.T, diff) / N_k[k]    # (D, D)

            cov_k += self.reg_covar * np.eye(D)
            new_covs[k] = cov_k

        self.covariances_ = new_covs

    def _compute_log_likelihood(self, X):
        """
        log L = sum_n log( sum_k π_k N(x_n|μ_k,Σ_k) )
        """
        N, _ = X.shape
        K = self.n_components
        likelihood = np.zeros((N, K))

        for k in range(K):
            likelihood[:, k] = self.weights_[k] * self._multivariate_gaussian(
                X,
                self.means_[k],
                self.covariances_[k],
            )

        total_likelihood = np.sum(likelihood, axis=1)
        total_likelihood[total_likelihood == 0] = 1e-12

        log_likelihood = np.sum(np.log(total_likelihood))
        return log_likelihood

    def fit(self, X):
        """
        用 EM 來學參數
        """
        X = np.asarray(X)
        self._initialize_params(X)

        prev_log_likelihood = None

        for i in range(self.max_iter):
            resp = self._e_step(X)
            self._m_step(X, resp)

            curr_log_likelihood = self._compute_log_likelihood(X)
            self.log_likelihood_ = curr_log_likelihood

            if prev_log_likelihood is not None:
                improvement = curr_log_likelihood - prev_log_likelihood
                if np.abs(improvement) < self.tol:
                    self.converged_ = True
                    break

            prev_log_likelihood = curr_log_likelihood

        return self

    def predict_proba(self, X):
        """
        回傳每個點屬於每個群的機率 (γ_{nk})
        """
        X = np.asarray(X)
        resp = self._e_step(X)
        return resp

    def predict(self, X):
        """
        回傳硬分群結果(最可能的群 index)
        """
        resp = self.predict_proba(X)
        return np.argmax(resp, axis=1)

if __name__ == "__main__":
    # 小型測試：隨機2群資料
    np.random.seed(42)

    mean1 = np.array([0, 0])
    cov1  = np.array([[1.0, 0.2],
                      [0.2, 0.5]])
    c1 = np.random.multivariate_normal(mean1, cov1, size=100)

    mean2 = np.array([4, 4])
    cov2  = np.array([[1.0, -0.3],
                      [-0.3, 0.8]])
    c2 = np.random.multivariate_normal(mean2, cov2, size=100)

    X = np.vstack([c1, c2])

    gmm = GaussianMixture(n_components=2, max_iter=200, random_state=0)
    gmm.fit(X)

    print("converged:", gmm.converged_)
    print("weights_:", gmm.weights_)
    print("means_:", gmm.means_)
    print("covariances_:", gmm.covariances_)
    print("first 5 proba:\n", gmm.predict_proba(X[:5]))
    print("first 5 labels:", gmm.predict(X[:5]))
```
