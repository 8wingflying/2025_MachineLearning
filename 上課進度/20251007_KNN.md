# 分類指標
- sklearn.metrics
  - https://scikit-learn.org/stable/api/sklearn.metrics.html 
- https://en.wikipedia.org/wiki/F-score
# K-Nearest Neighbors (KNN)
- K-Nearest Neighbors (KNN) 是一種 基於實例 (instance-based) 的學習方法。
- 核心思想是：「一個樣本的分類結果，取決於距離它最近的 K 個鄰居的分類。」
- 也就是說，當要預測一筆新資料時，KNN 不會先建立模型，而是：
- 計算新樣本與訓練集中所有樣本的距離；
- 找出距離最近的 K 個樣本；
- 以這些鄰居的多數類別 (分類) 或 平均值 (回歸) 來決定新樣本的預測結果。

## 程式實作
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1. 載入資料
iris = load_iris()
X, y = iris.data, iris.target

# 2. 分割訓練與測試集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 3. 標準化
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 4. 建立 KNN 模型
knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')
knn.fit(X_train, y_train)

# 5. 預測與評估
y_pred = knn.predict(X_test)
print("準確率:", accuracy_score(y_test, y_pred))
print("\n分類報告：\n", classification_report(y_test, y_pred))

```
## 程式實作
```python
"""
以物件導向(OOP)方式實作 KNN 演算法，並以 Iris 資料集為例
作者：T Ben
"""

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

class KNNClassifier:
    """
    手動實作 K-Nearest Neighbors 分類器
    """
    def __init__(self, k=3):
        """
        初始化
        :param k: 鄰居數量
        """
        self.k = k
        self.X_train = None
        self.y_train = None

    def _euclidean_distance(self, x1, x2):
        """計算歐式距離"""
        return np.sqrt(np.sum((x1 - x2) ** 2))

    def fit(self, X, y):
        """儲存訓練資料（KNN 無需訓練過程）"""
        self.X_train = X
        self.y_train = y

    def _predict_one(self, x):
        """對單一樣本預測其類別"""
        # 計算與所有訓練樣本的距離
        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]
        # 取出最近的 k 個鄰居索引
        k_indices = np.argsort(distances)[:self.k]
        # 對應的標籤
        k_neighbor_labels = [self.y_train[i] for i in k_indices]
        # 多數表決
        most_common = np.bincount(k_neighbor_labels).argmax()
        return most_common

    def predict(self, X):
        """對多筆測試資料進行預測"""
        return np.array([self._predict_one(x) for x in X])


# === 主程式 ===
if __name__ == "__main__":
    # 1. 載入資料
    iris = load_iris()
    X, y = iris.data, iris.target

    # 2. 分割訓練與測試資料
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    # 3. 特徵標準化
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # 4. 建立與訓練模型
    knn = KNNClassifier(k=5)
    knn.fit(X_train, y_train)

    # 5. 預測
    y_pred = knn.predict(X_test)

    # 6. 評估
    print("準確率:", accuracy_score(y_test, y_pred))
    print("\n分類報告：\n", classification_report(y_test, y_pred, target_names=iris.target_names))

```
