# ğŸ¤“ æ··åˆæ³›åŒ– (Blending) æ•™å­¸æ–‡ä»¶

## 1ï¸âƒ£ æ··åˆæ³›åŒ–ç°¡ä»‹

**æ··åˆæ³›åŒ–ï¼ˆBlendingï¼‰** æ˜¯ä¸€ç¨® **é›†æˆå­¸ç¿’ (Ensemble Learning)** æŠ€è¡“ï¼Œç”¨æ–¼çµåˆå¤šå€‹åŸºæ¨¡å‹ï¼ˆBase Modelsï¼‰çš„é æ¸¬çµæœï¼Œä»¥æå‡æ¨¡å‹æ•´é«”è¡¨ç¾ã€‚  
èˆ‡ Stacking é¡ä¼¼ï¼ŒBlending ä¹Ÿå¼•å…¥ä¸€å€‹ã€Œæ¬¡ç´šæ¨¡å‹ (Meta Model)ã€ï¼Œä½†è¨“ç·´æ–¹å¼ç•¥æœ‰ä¸åŒï¼š

| æ¯”è¼ƒé …ç›® | Stacking | Blending |
|-----------|-----------|-----------|
| ç¬¬äºŒå±¤è³‡æ–™ä¾†æº | äº¤å‰é©—è­‰é æ¸¬ (out-of-fold prediction) | ç•™å‡ºé©—è­‰é›† (hold-out set) é æ¸¬ |
| è¨“ç·´é›†åˆ†å‰² | K-Fold | è¨“ç·´é›† / é©—è­‰é›† åˆ†å‰² |
| è¨ˆç®—é‡ | é«˜ (å¤šæ¬¡äº¤å‰é©—è­‰) | è¼ƒä½ |
| éæ“¬åˆé¢¨éšª | ä½ | ç¨é«˜ (å–æ±ºæ–¼é©—è­‰é›†å¤§å°) |

---

## 2ï¸âƒ£ åŸç†æ¦‚è¿°

å‡è¨­æˆ‘å€‘æœ‰ä¸‰å€‹åŸºæ¨¡å‹ï¼š
- æ¨¡å‹ 1ï¼š`LinearRegression`
- æ¨¡å‹ 2ï¼š`RandomForestRegressor`
- æ¨¡å‹ 3ï¼š`GradientBoostingRegressor`

æ­¥é©Ÿï¼š

1. è¨“ç·´é›†å’Œé©—è­‰é›†åˆ†å‰²
2. è¨“ç·´åŸºæ¨¡å‹
3. ç”¨é©—è­‰é›†ç”¢ç”Ÿ meta-features
4. è¼¸å…¥ meta model è¨“ç·´
5. æœ€çµ‚é æ¸¬

$$
\hat{y} = f_{\text{meta}}(f_1(x), f_2(x), ..., f_n(x))
$$

---

## 3ï¸âƒ£ Python å¯¦ä½œï¼šè¿´æ­¸ç‰ˆ

(å…¨éƒ¨ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨åŒä¸Šç‰ˆ)

---

## 4ï¸âƒ£ åˆ†é¡ç‰ˆ (Classification Blending)

### 4.1 è³‡æ–™

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, roc_auc_score

X, y = load_breast_cancer(return_X_y=True)
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)
```

### 4.2 åŸºæ¨¡å‹

```python
models = [
    ("lr", LogisticRegression(max_iter=1000)),
    ("rf", RandomForestClassifier(n_estimators=200, random_state=42)),
    ("gb", GradientBoostingClassifier(random_state=42))
]
for _, model in models:
    model.fit(X_train, y_train)
```

### 4.3 Meta features

```python
meta_features = np.column_stack([
    model.predict_proba(X_valid)[:, 1] for _, model in models
])
meta_model = LogisticRegression()
meta_model.fit(meta_features, y_valid)
final_prob = meta_model.predict_proba(meta_features)[:, 1]
final_pred = (final_prob > 0.5).astype(int)
```

### 4.4 è©•ä¼°

```python
acc = accuracy_score(y_valid, final_pred)
auc = roc_auc_score(y_valid, final_prob)
print(f"Blending Accuracy: {acc:.4f}")
print(f"Blending AUC: {auc:.4f}")
```

---

## 5ï¸âƒ£ K-Fold Blending é€²éšæŠ€å·§

### åŸç†

ä½¿ç”¨ K-Fold Cross-Validation ç”¢ç”Ÿ Out-of-Fold (OOF) é æ¸¬ï¼Œå¯æ¸›å°‘é©—è­‰é›†çš„åƒ¹å€¼æå¤±ã€‚

### 5.1 Python ç¨‹å¼

```python
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.datasets import load_boston

X, y = load_boston(return_X_y=True)
models = [
    ("lr", LinearRegression()),
    ("rf", RandomForestRegressor(random_state=42)),
    ("gb", GradientBoostingRegressor(random_state=42))
]

kf = KFold(n_splits=5, shuffle=True, random_state=42)
meta_features = np.zeros((len(X), len(models)))

for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):
    X_tr, X_va = X[train_idx], X[valid_idx]
    y_tr, y_va = y[train_idx], y[valid_idx]

    for i, (_, model) in enumerate(models):
        model.fit(X_tr, y_tr)
        meta_features[valid_idx, i] = model.predict(X_va)

meta_model = LinearRegression()
meta_model.fit(meta_features, y)
```

---

## 6ï¸âƒ£ å„ªç¼ºé»

| å„ªé» | ç¼ºé» |
|------|------|
| è¨ˆç®—æ•ˆç‡é«˜ | é©—è­‰é›†æ•ˆæ‡‰æ•æ„Ÿ |
| æ˜“æ‡‚æ˜“ç”¨ | é«˜ç›¸é—œåŸºæ¨¡å‹æ•ˆæœä¸ä½³ |
| å¯å¼·åŒ–æ¨¡å‹å¤šæ¨£æ€§ | éœ€ç¶“é©—è­‰æ¯”ä¾‹è¨­è¨ˆ |

---

## 7ï¸âƒ£ å¯¦å‹™å»ºè­°

- é©—è­‰æ¯”ä¾‹ 20~30%
- å¤šæ¨£æ€§åŸºæ¨¡å‹ (ç·šæ€§ + æ¨¹å‹)
- Meta model å¯ç”¨ XGBoost / LightGBM
- åˆ†æåŸºæ¨¡å‹ç›¸é—œæ€§

---

## 8ï¸âƒ£ çµè«–

Blending æ˜¯ **é«˜æ•ˆã€å¿«é€Ÿã€ç²¾æº–çš„é›†æˆæ¨¡å‹æ–¹æ³•**ï¼Œå¦‚æœè³‡æ–™é‡å¤§ï¼Œå¯è€ƒæ…®ä½¿ç”¨ **Stacking (Cross-Validation)** å½¢å¼ä»¥æå‡æ³›åŒ–æ€§èƒ½ã€‚

---

## ğŸ“š å¾ŒçºŒè®€ç‰©

- Wolpert, D. H. (1992). *Stacked Generalization*. Neural Networks.  
- GÃ©ron, A. (2023). *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*.  
- scikit-learn Docs: https://scikit-learn.org/stable/

