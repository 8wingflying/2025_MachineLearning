# ğŸ“˜ Blending vs Stacking æ•™å­¸æ–‡ä»¶ï¼ˆç¹é«”ä¸­æ–‡ï¼‹è‹±æ–‡å°ç…§ï¼‹åœ–è¡¨ï¼‹Python å¯¦ä½œï¼‰

---

## ğŸ¤  1. å°è«–ï½œIntroduction

### ä¸­æ–‡ï¼š
åœ¨é›†æˆå­¸ç¿’ï¼ˆEnsemble Learningï¼‰ä¸­ï¼Œ**Blendingï¼ˆæ··åˆæ³›åŒ–ï¼‰** èˆ‡ **Stackingï¼ˆå †ç–Šæ³›åŒ–ï¼‰** éƒ½å±¬æ–¼ã€Œå¤šæ¨¡å‹èåˆã€æŠ€è¡“ã€‚  
å®ƒå€‘é€éçµåˆå¤šå€‹åŸºæ¨¡å‹ï¼ˆBase Modelsï¼‰çš„é æ¸¬çµæœä¾†æå‡æœ€çµ‚æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚  
å…©è€…çš„ä¸»è¦å·®ç•°åœ¨æ–¼ã€Œå¦‚ä½•åˆ©ç”¨è¨“ç·´è³‡æ–™ã€ä»¥åŠã€Œå¦‚ä½•ç”¢ç”Ÿç¬¬äºŒå±¤æ¨¡å‹çš„è¼¸å…¥ã€ã€‚

### English:
In ensemble learning, **Blending** and **Stacking** are both meta-learning techniques for combining multiple base models to enhance generalization.  
Their main difference lies in how they handle training data and generate meta-model inputs.

---

## ğŸ¥‰ 2. åŸºæœ¬æ¦‚å¿µæ¯”è¼ƒï½œCore Concepts Comparison

| é …ç›® | **Stackingï¼ˆå †ç–Šæ³›åŒ–ï¼‰** | **Blendingï¼ˆæ··åˆæ³›åŒ–ï¼‰** |
|------|----------------------------|----------------------------|
| **ä¸»è¦åŸç† / Core Idea** | ä½¿ç”¨ K-Fold Cross-Validation å»ºç«‹ out-of-fold é æ¸¬ï¼Œè¨“ç·´ meta-modelã€‚ | ä½¿ç”¨ä¸€éƒ¨åˆ†é©—è­‰é›† (holdout set) ä¾†è¨“ç·´ meta-modelã€‚ |
| **è³‡æ–™åˆ©ç”¨æ–¹å¼ / Data Usage** | å…¨éƒ¨è¨“ç·´è³‡æ–™çš†è¢«ä½¿ç”¨ï¼ˆé€éäº¤å‰é©—è­‰ï¼‰ã€‚ | ä¸€éƒ¨åˆ†è³‡æ–™ç•™ä½œé©—è­‰ï¼Œæœªç”¨æ–¼åŸºæ¨¡å‹è¨“ç·´ã€‚ |
| **æ³›åŒ–èƒ½åŠ› / Generalization** | é«˜ï¼ˆå› äº¤å‰é©—è­‰é¿å…éæ“¬åˆï¼‰ã€‚ | è¼ƒæ˜“ overfitï¼ˆè‹¥ holdout å¤ªå°ï¼‰ã€‚ |
| **è¨“ç·´é€Ÿåº¦ / Speed** | è¼ƒæ…¢ï¼ˆå¤šæ¬¡è¨“ç·´ï¼‰ã€‚ | è¼ƒå¿«ï¼ˆä¸€æ¬¡è¨“ç·´å³å¯ï¼‰ã€‚ |
| **å¯¦ä½œé›£åº¦ / Complexity** | è¼ƒé«˜ | è¼ƒä½ |
| **é©ç”¨å ´æ™¯ / Use Case** | éœ€è¦æœ€å¼·æ³›åŒ–èƒ½åŠ›çš„æƒ…å¢ƒï¼ˆå¦‚ç«¶è³½æœ€çµ‚æ¨¡å‹ï¼‰ã€‚ | å¿«é€Ÿæ¸¬è©¦æˆ–å°å‹è³‡æ–™é›†ã€‚ |

---

## âš™ï¸ 3. é‹ä½œæµç¨‹åœ–è§£ï½œWorkflow Diagrams

### ASCII ç¤ºæ„ï¼š
```
Stacking (with K-Fold CV):              Blending (with Holdout Set):

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”Œ                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”Œ
 â”‚ Train Data â”‚                       â”‚ Train Data â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚                                   â”‚
     â”Œâ”€â–¼â”€â”                             â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
     â”‚K-Foldâ”‚                         â”‚Train+Valâ”‚
     â””â”€â”¬â”€â”¬â”€â”˜                             â””â”€â”€â”€â”¬â”€â”€â”€â”˜
       â”‚                                   â”‚
  Base Models                          Base Models
       â”‚                                   â”‚
  Out-of-Fold Preds                 Holdout Predictions
       â”‚                                   â”‚
     Meta Model â†’ Final Pred         Meta Model â†’ Final Pred
```

---

### ğŸ¥® Matplotlib è¦–è¦ºåŒ–æµç¨‹åœ–ç¨‹å¼ç¢¼

```python
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

fig, ax = plt.subplots(figsize=(10, 5))

# å€å¡Šé¡è‰²
train_color = "#AED6F1"
model_color = "#A9DFBF"
meta_color = "#F9E79F"

# Stacking å€åŸŸ
ax.add_patch(mpatches.Rectangle((0.1, 0.6), 0.3, 0.25, color=train_color))
ax.text(0.25, 0.72, "Training Data\n(K-Fold)", ha='center', fontsize=10)

ax.add_patch(mpatches.Rectangle((0.45, 0.6), 0.25, 0.25, color=model_color))
ax.text(0.57, 0.72, "Base Models\n(out-of-fold preds)", ha='center', fontsize=10)

ax.add_patch(mpatches.Rectangle((0.75, 0.6), 0.15, 0.25, color=meta_color))
ax.text(0.83, 0.72, "Meta Model", ha='center', fontsize=10)

ax.arrow(0.4, 0.725, 0.05, 0, head_width=0.03, head_length=0.03, fc='k', ec='k')
ax.arrow(0.7, 0.725, 0.05, 0, head_width=0.03, head_length=0.03, fc='k', ec='k')
ax.text(0.5, 0.85, "Stacking Flow", fontsize=12, weight='bold')

# Blending å€åŸŸ
ax.add_patch(mpatches.Rectangle((0.1, 0.15), 0.3, 0.25, color=train_color))
ax.text(0.25, 0.27, "Training + Holdout Data", ha='center', fontsize=10)

ax.add_patch(mpatches.Rectangle((0.45, 0.15), 0.25, 0.25, color=model_color))
ax.text(0.57, 0.27, "Base Models\n(on Train)", ha='center', fontsize=10)

ax.add_patch(mpatches.Rectangle((0.75, 0.15), 0.15, 0.25, color=meta_color))
ax.text(0.83, 0.27, "Meta Model\n(on Holdout)", ha='center', fontsize=10)

ax.arrow(0.4, 0.25, 0.05, 0, head_width=0.03, head_length=0.03, fc='k', ec='k')
ax.arrow(0.7, 0.25, 0.05, 0, head_width=0.03, head_length=0.03, fc='k', ec='k')
ax.text(0.5, 0.4, "Blending Flow", fontsize=12, weight='bold')

ax.set_xlim(0, 1)
ax.set_ylim(0, 1)
ax.axis('off')
plt.show()
```

---

## ğŸ¥® 4. Python å¯¦ä½œç¤ºä¾‹ï½œPython Implementation

### ï¼ˆAï¼‰Stacking å¯¦ä½œï¼ˆscikit-learnï¼‰
```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# è³‡æ–™è¼‰å…¥
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# åŸºæ¨¡å‹
base_models = [
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('gb', GradientBoostingClassifier(random_state=42))
]

# å…ƒæ¨¡å‹
meta_model = LogisticRegression()
stack_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)

# è¨“ç·´èˆ‡é æ¸¬
stack_model.fit(X_train, y_train)
y_pred = stack_model.predict(X_test)

print("Stacking æº–ç¢ºç‡:", accuracy_score(y_test, y_pred))
```

---

### ï¼ˆBï¼‰Blending å¯¦ä½œï¼ˆè‡ªè¨‚ holdoutï¼‰
```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_breast_cancer

# è³‡æ–™åˆ†å‰²ï¼štrain / holdout / test
X, y = load_breast_cancer(return_X_y=True)
X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42)
X_holdout_train, X_test, y_holdout_train, y_test = train_test_split(X_holdout, y_holdout, test_size=0.5, random_state=42)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
gb = GradientBoostingClassifier(random_state=42)

rf.fit(X_train, y_train)
gb.fit(X_train, y_train)

# ç”¢ç”Ÿ holdout é æ¸¬
holdout_pred = np.column_stack([
    rf.predict_proba(X_holdout_train)[:,1],
    gb.predict_proba(X_holdout_train)[:,1]
])

meta = LogisticRegression()
meta.fit(holdout_pred, y_holdout_train)

# æ¸¬è©¦éšæ®µ
test_pred = np.column_stack([
    rf.predict_proba(X_test)[:,1],
    gb.predict_proba(X_test)[:,1]
])
final_pred = meta.predict(test_pred)

print("Blending æº–ç¢ºç‡:", accuracy_score(y_test, final_pred))
```

---

## ğŸ—¾ 5. å„ªç¼ºé»å°ç…§è¡¨ï½œPros & Cons

| é …ç›® | **Stacking** | **Blending** |
|------|---------------|---------------|
| æ³›åŒ–èƒ½åŠ› | âœ… é«˜ï¼ˆå› äº¤å‰é©—è­‰ï¼‰ | âš ï¸ ä¸­ç­‰ï¼Œè¦– holdout å¤§å° |
| è¨ˆç®—æˆæœ¬ | âŒ é«˜ | âœ… ä½ |
| è¨“ç·´é€Ÿåº¦ | âŒ è¼ƒæ…¢ | âœ… å¿«é€Ÿ |
| å¯¦ä½œç°¡æ˜“åº¦ | âŒ è¤‡é›œ | âœ… ç°¡å–® |
| è³‡æ–™åˆ©ç”¨ç‡ | âœ… é«˜ | âŒ éƒ¨åˆ†è³‡æ–™æœªç”¨æ–¼è¨“ç·´ |
| éæ“¬åˆé¢¨éšª | âœ… ä½ | âš ï¸ è¼ƒé«˜ |
| é©åˆè³‡æ–™é‡ | å¤§å‹è³‡æ–™ | å°å‹è³‡æ–™æˆ–å¿«é€Ÿå¯¦é©— |

---

## ğŸ¦¯ 6. ä½¿ç”¨å»ºè­°ï½œBest Practice Recommendations

| æ¢ä»¶ | å»ºè­°ä½¿ç”¨ |
|------|-----------|
| è³‡æ–™é‡å¤§ä¸”ç›®æ¨™æ˜¯æœ€çµ‚é«˜ç²¾åº¦æ¨¡å‹ | **Stacking** |
| éœ€è¦å¿«é€Ÿæ¸¬è©¦æ¨¡å‹çµ„åˆ

