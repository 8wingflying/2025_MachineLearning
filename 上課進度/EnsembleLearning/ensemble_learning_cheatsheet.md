# ğŸ§  Ensemble Learning Cheatsheet

## ğŸ“˜ å››å¤§ä¸»æµé›†æˆå­¸ç¿’ç­–ç•¥
| é¡å‹ | ä¸­æ–‡åç¨± | æ ¸å¿ƒæ¦‚å¿µ | å¸¸è¦‹æ¨¡å‹ | ä¸»è¦å„ªå‹¢ |
|------|-----------|-----------|------------|-----------|
| **Bagging** | è£è¢‹æ³• | å¤šæ¨¡å‹åœ¨éš¨æ©Ÿå–æ¨£çš„è³‡æ–™ä¸Šè¨“ç·´å¾Œå¹³å‡ | RandomForest | ç©©å®šã€æŠ—éæ“¬åˆ |
| **Boosting** | æå‡æ³• | é€æ­¥ä¿®æ­£å‰ä¸€æ¨¡å‹çš„éŒ¯èª¤ | AdaBoost / XGBoost / LightGBM | é«˜æº–ç¢ºç‡ã€å¯èª¿æ•´æ¬Šé‡ |
| **Stacking** | å †ç–Šæ³• | ä¸åŒæ¨¡å‹è¼¸å‡ºçµ„åˆæˆæ¬¡ç´šæ¨¡å‹ | StackingClassifier | æ¨¡å‹äº’è£œã€éˆæ´»æ€§é«˜ |
| **Blending** | æ··åˆæ³• | ç”¨é©—è­‰é›†åŠ æ¬Šèåˆæ¨¡å‹é æ¸¬ | è‡ªå®š Logistic Regression | ç°¡å–®å¿«é€Ÿã€é©åˆ Kaggle |

---

## âš™ï¸ å¸¸ç”¨ Python å¥—ä»¶
| å¥—ä»¶åç¨± | åŠŸèƒ½ | å‚™è¨» |
|-----------|------|------|
| `scikit-learn` | Bagging / Boosting / Stacking | æœ€å®Œæ•´çš„é›†æˆæ–¹æ³•é›†åˆ |
| `xgboost` | é«˜æ•ˆæ¢¯åº¦æå‡æ¨¹ | GPU æ”¯æ´ã€æ¥­ç•Œæ¨™æº– |
| `lightgbm` | å¿«é€Ÿæ¢¯åº¦æå‡ | é©åˆå¤§å‹è¡¨æ ¼è³‡æ–™ |
| `catboost` | é¡åˆ¥ç‰¹å¾µå‹å¥½ | ç„¡éœ€ One-Hot ç·¨ç¢¼ |
| `mlens` / `vecstack` | Stacking / Blending å·¥å…· | sklearn ç›¸å®¹æ“´å…… |

---

## ğŸ”¢ Bagging ç¯„ä¾‹ï¼ˆRandom Forestï¼‰
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
rf_model = Pipeline([
    ("preprocess", preprocess),
    ("model", RandomForestClassifier(n_estimators=300, random_state=42))
])
rf_model.fit(X_train, y_train)
```

---

## ğŸš€ Boosting ç¯„ä¾‹
### AdaBoost
```python
from sklearn.ensemble import AdaBoostClassifier
ada_model = Pipeline([
    ("preprocess", preprocess),
    ("model", AdaBoostClassifier(n_estimators=200, learning_rate=0.8, random_state=42))
])
```

### XGBoost
```python
import xgboost as xgb
xgb_model = Pipeline([
    ("preprocess", preprocess),
    ("model", xgb.XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=4, random_state=42))
])
```

### LightGBM
```python
import lightgbm as lgb
lgb_model = Pipeline([
    ("preprocess", preprocess),
    ("model", lgb.LGBMClassifier(n_estimators=400, learning_rate=0.05, random_state=42))
])
```

---

## ğŸ§  Stacking ç¯„ä¾‹
```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
base_models = [
    ('rf', RandomForestClassifier(n_estimators=200)),
    ('xgb', xgb.XGBClassifier(n_estimators=300)),
]
stack_model = Pipeline([
    ("preprocess", preprocess),
    ("model", StackingClassifier(estimators=base_models, final_estimator=LogisticRegression()))
])
```

---

## ğŸ”„ Blending ç¯„ä¾‹
```python
from sklearn.linear_model import LogisticRegression
rf_model.fit(X_tr, y_tr)
xgb_model.fit(X_tr, y_tr)
val_rf = rf_model.predict_proba(X_val)[:,1]
val_xgb = xgb_model.predict_proba(X_val)[:,1]
blend_X_val = np.vstack([val_rf, val_xgb]).T
blender = LogisticRegression().fit(blend_X_val, y_val)
```

---

## ğŸ“Š è©•ä¼°èˆ‡è¦–è¦ºåŒ– (Matplotlib + Seaborn)
```python
import seaborn as sns, matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix

models = {'RF': y_pred_rf, 'XGB': y_pred_xgb, 'LGB': y_pred_lgb}
acc = [accuracy_score(y_test, y_pred) for y_pred in models.values()]
sns.barplot(x=list(models.keys()), y=acc, palette='viridis')
plt.title('Accuracy Comparison')
plt.show()
```

---

## ğŸ§© å¿«é€Ÿå»ºè­°å°ç…§è¡¨
| å ´æ™¯ | æ¨è–¦æ–¹æ³• | å¥—ä»¶ |
|------|------------|------|
| å°å‹è¡¨æ ¼è³‡æ–™ | Bagging / Stacking | sklearn |
| å¤§å‹è¡¨æ ¼è³‡æ–™ | Boosting | XGBoost / LightGBM |
| è‡ªå‹•åŒ–å¯¦é©— | AutoML | PyCaret / AutoGluon |
| æ·±åº¦å­¸ç¿’èåˆ | Blending | TensorFlow / PyTorch + sklearn |

---

âœ… **é€ŸæŸ¥æç¤º**ï¼š
- Bagging ç©©å®šæ¨¡å‹ã€Boosting æå‡æº–ç¢ºç‡
- Stacking + Blending ç‚ºé«˜éšèåˆç­–ç•¥
- XGBoost å¹¾ä¹æ˜¯æ‰€æœ‰ Kaggle æ¯”è³½çš„æ ¸å¿ƒæ­¦å™¨