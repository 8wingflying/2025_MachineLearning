# Distribution-based Clustering æ•™å­¸æ–‡ä»¶
*Version: v2 ï¼ˆå« Gaussian Mixture Model å¯¦ä½œèˆ‡åˆ†å¸ƒè¦–è¦ºåŒ–åœ–ç¯„ä¾‹ï¼‰*

---

## ğŸ“˜ ä¸€ã€åˆ†å¸ƒå¼åˆ†ç¾¤ï¼ˆDistribution-based Clusteringï¼‰ç°¡ä»‹

**Distribution-based Clusteringï¼ˆåˆ†å¸ƒå¼åˆ†ç¾¤ï¼‰** æ˜¯ä»¥ã€Œæ©Ÿç‡åˆ†å¸ƒæ¨¡å‹ã€ç‚ºåŸºç¤çš„åˆ†ç¾¤æ–¹æ³•ï¼Œå‡è¨­è³‡æ–™æ˜¯ç”±å¤šå€‹ä¸åŒçš„æ©Ÿç‡åˆ†å¸ƒï¼ˆå¦‚é«˜æ–¯åˆ†å¸ƒï¼‰æ‰€ç”Ÿæˆã€‚é€éä¼°è¨ˆé€™äº›åˆ†å¸ƒçš„åƒæ•¸ï¼Œèƒ½å¤ æ‰¾å‡ºéš±è—çš„ç¾¤é›†çµæ§‹ã€‚

å¸¸è¦‹æ¼”ç®—æ³•ï¼š
- Gaussian Mixture Model (**GMM**)
- Expectation-Maximization (**EM Algorithm**)

---

## ğŸ§© äºŒã€ç†è«–åŸºç¤

### 1ï¸âƒ£ ä¸»è¦æ€æƒ³
å‡è¨­è³‡æ–™é›† \( X = \{x_1, x_2, ..., x_n\} \) æ˜¯ç”± \( K \) å€‹åˆ†å¸ƒç”¢ç”Ÿï¼Œå‰‡æ•´é«”åˆ†å¸ƒå¯è¡¨ç¤ºç‚ºï¼š

\[
P(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)
\]

å…¶ä¸­ï¼š
- \( \pi_k \)ï¼šç¬¬ \( k \) å€‹åˆ†å¸ƒçš„æ··åˆæ¬Šé‡ï¼ˆ\( \sum \pi_k = 1 \)ï¼‰
- \( \mu_k \)ï¼šç¬¬ \( k \) å€‹é«˜æ–¯åˆ†å¸ƒçš„å¹³å‡å€¼
- \( \Sigma_k \)ï¼šç¬¬ \( k \) å€‹é«˜æ–¯åˆ†å¸ƒçš„å…±è®Šç•°æ•¸çŸ©é™£

---

## âš™ï¸ ä¸‰ã€Expectation-Maximizationï¼ˆEMï¼‰æ¼”ç®—æ³•æ­¥é©Ÿ

1. **E æ­¥é©Ÿï¼ˆExpectationï¼‰**ï¼šæ ¹æ“šç•¶å‰åƒæ•¸ï¼Œè¨ˆç®—æ¯å€‹æ¨£æœ¬å±¬æ–¼å„åˆ†å¸ƒçš„æ©Ÿç‡ã€‚
2. **M æ­¥é©Ÿï¼ˆMaximizationï¼‰**ï¼šæ›´æ–°åˆ†å¸ƒåƒæ•¸ï¼Œä½¿å¾—è§€å¯Ÿè³‡æ–™çš„ä¼¼ç„¶å€¼æœ€å¤§åŒ–ã€‚
3. **é‡è¤‡ E-M** ç›´åˆ°æ”¶æ–‚ã€‚

---

## ğŸ§® å››ã€Gaussian Mixture Modelï¼ˆGMMï¼‰æ•¸å­¸å…¬å¼

é«˜æ–¯åˆ†å¸ƒçš„æ©Ÿç‡å¯†åº¦å‡½æ•¸ï¼š

\[
\mathcal{N}(x \mid \mu, \Sigma) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \right)
\]

æ•´é«”æ¨¡å‹ç‚ºåŠ æ¬Šå’Œï¼š

\[
P(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k)
\]

---

## ğŸ§  äº”ã€Python å¯¦ä½œï¼ˆGaussian Mixture Modelï¼‰

```python
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
import seaborn as sns

# ç”Ÿæˆæ¨£æœ¬è³‡æ–™
X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)

# å»ºç«‹ GMM æ¨¡å‹
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=0)
gmm.fit(X)
labels = gmm.predict(X)

# ç¹ªåœ–
plt.figure(figsize=(7,5))
sns.scatterplot(x=X[:,0], y=X[:,1], hue=labels, palette='deep', s=40)
plt.title("Gaussian Mixture Model Clustering Result")
plt.show()
```

---

## ğŸ“ˆ å…­ã€GMM åˆ†å¸ƒè¦–è¦ºåŒ–åœ–ï¼ˆMatplotlib è¼¸å‡ºï¼‰

### âœ… ä½¿ç”¨æ©¢åœ“å¯è¦–åŒ–é«˜æ–¯åˆ†å¸ƒç¯„åœ

```python
import numpy as np
from matplotlib.patches import Ellipse

def draw_ellipse(position, covariance, ax=None, **kwargs):
    ax = ax or plt.gca()
    if covariance.shape == (2, 2):
        U, s, Vt = np.linalg.svd(covariance)
        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width, height = 2 * np.sqrt(s)
    else:
        angle, width, height = 0, 2 * np.sqrt(covariance), 2 * np.sqrt(covariance)
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs))

plt.figure(figsize=(8,6))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')
for pos, covar in zip(gmm.means_, gmm.covariances_):
    draw_ellipse(pos, covar, alpha=0.3, color='red')
plt.title('Gaussian Mixture Model - Distribution Visualization')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
```

ğŸ“Š **è¼¸å‡ºè§£é‡‹ï¼š**
- æ¯å€‹ç´…è‰²æ©¢åœ“ä»£è¡¨ä¸€å€‹é«˜æ–¯åˆ†å¸ƒçš„ç¯„åœã€‚
- é»é¡è‰²ä»£è¡¨æ‰€å±¬ç¾¤é›†ã€‚

---

## ğŸ“Š ä¸ƒã€GMM èˆ‡ K-Means æ¯”è¼ƒ

| ç‰¹æ€§ | GMM | K-Means |
|------|------|---------|
| æ¨¡å‹å‡è¨­ | æ©Ÿç‡åˆ†å¸ƒ | å¹¾ä½•è·é›¢ |
| åˆ†ç¾¤é‚Šç•Œ | è»Ÿåˆ†ç¾¤ï¼ˆSoftï¼‰ | ç¡¬åˆ†ç¾¤ï¼ˆHardï¼‰ |
| æ”¯æ´æ©¢åœ“å½¢ç¾¤é›† | âœ… æ˜¯ | âŒ å¦ |
| é›¢ç¾¤é»è™•ç† | è¼ƒä½³ | ä¸€èˆ¬ |
| è¼¸å‡ºçµæœ | æ¯é»çš„ç¾¤é›†æ©Ÿç‡ | ç¾¤é›†æ¨™ç±¤ |

---

## ğŸ§© å…«ã€æ‡‰ç”¨æ¡ˆä¾‹

- é‡‘èé¢¨éšªæ¨¡å‹ä¸­çš„å®¢ç¾¤åˆ†å±¤
- èªéŸ³è¾¨è­˜ä¸­çš„è²å­¸æ¨¡å‹
- åœ–åƒåˆ†å‰²ï¼ˆä¾‹å¦‚è†šè‰²å€åŸŸåµæ¸¬ï¼‰
- æ–‡æœ¬ä¸»é¡Œåˆ†ç¾¤ï¼ˆä»¥è©åµŒå…¥å¾Œå¥—ç”¨ GMMï¼‰

---

## ğŸ§® ä¹ã€å„ªç¼ºé»æ•´ç†

| å„ªé» | ç¼ºé» |
|------|------|
| å¯å»ºæ¨¡ä»»æ„å½¢ç‹€ç¾¤é›† | éœ€æŒ‡å®šç¾¤æ•¸ |
| æä¾›æ©Ÿç‡å¼åˆ†ç¾¤çµæœ | å°åˆå§‹å€¼æ•æ„Ÿ |
| å¯è¦–åŒ–å¯ç†è§£åº¦é«˜ | é«˜ç¶­è³‡æ–™é‹ç®—é‡å¤§ |

---

## ğŸ“š åã€å»¶ä¼¸ç·´ç¿’

1. å˜—è©¦ä¸åŒçš„ `covariance_type`ï¼ˆ`full`, `tied`, `diag`, `spherical`ï¼‰ã€‚
2. å°‡ GMM å¥—ç”¨æ–¼é™ç¶­å¾Œçš„è³‡æ–™ï¼ˆä¾‹å¦‚ PCA çµæœï¼‰ã€‚
3. èˆ‡ DBSCAN æ¯”è¼ƒåˆ†ç¾¤æ•ˆæœã€‚
4. åœ¨ç•°å¸¸åµæ¸¬ä»»å‹™ä¸­åˆ©ç”¨ GMM çš„æ©Ÿç‡é–¾å€¼ä½œç‚ºåˆ¤æ–·ä¾æ“šã€‚

---

## ğŸ§¾ åä¸€ã€åƒè€ƒè³‡æ–™

- scikit-learn å®˜æ–¹æ–‡ä»¶ï¼š[https://scikit-learn.org/stable/modules/mixture.html](https://scikit-learn.org/stable/modules/mixture.html)
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning.* Springer.
- Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective.* MIT Press.

---

ğŸ“¦ æª”æ¡ˆåç¨±å»ºè­°ï¼š
```
DISTRIBUTION_based_Clustering.md
```

ğŸ“¦ åŸ·è¡Œç’°å¢ƒï¼š
```bash
pip install scikit-learn matplotlib seaborn numpy
```

