# Gaussian Mixture Model (GMM) æ•™å­¸æ–‡ä»¶-Streamlit äº’å‹•ç‰ˆ

---

## ğŸ§­ ä¸€ã€GMM æ¦‚å¿µç°¡ä»‹ | GMM Overview
 
é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGaussian Mixture Model, GMMï¼‰æ˜¯ä¸€ç¨®ä»¥å¤šå€‹é«˜æ–¯åˆ†ä½ˆçµ„æˆçš„æ©Ÿç‡æ¨¡å‹ï¼Œç”¨æ–¼å°è³‡æ–™é€²è¡Œç¾¤é›†åˆ†æèˆ‡å¯†åº¦ä¼°è¨ˆã€‚

---

## ğŸ“˜ äºŒã€æ•¸å­¸å®šç¾© | Mathematical Definition

$$
p(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x|\mu_k, \Sigma_k)
$$ 

**è§£é‡‹ï¼š**  
å…¶ä¸­ $\pi_k$ ç‚ºæ¬Šé‡ï¼ˆæ‰€æœ‰ç¾¤é›†æ¬Šé‡åŠ ç¸½ç‚º 1ï¼‰ï¼Œ $$\mu_k $$ ç‚ºå‡å€¼ï¼Œ $\Sigma_k $ ç‚ºå…±è®Šç•°æ•¸çŸ©é™£ã€‚

---

## âš™ï¸ ä¸‰ã€EM æ¼”ç®—æ³• | EM Algorithm

### E-step

$$ 
\gamma_{ik} = \frac{\pi_k \mathcal{N}(x_i|\mu_k, \Sigma_k)}{\sum_{j}\pi_j \mathcal{N}(x_i|\mu_j, \Sigma_j)}
$$ 

### M-step

$$ 
\pi_k = \frac{N_k}{N}, \quad \mu_k = \frac{1}{N_k}\sum_i \gamma_{ik}x_i, \quad \Sigma_k = \frac{1}{N_k}\sum_i \gamma_{ik}(x_i-\mu_k)(x_i-\mu_k)^T
$$ 

**ä¸­æ–‡èªªæ˜ï¼š**  
åè¦†åŸ·è¡Œ E èˆ‡ M æ­¥é©Ÿç›´åˆ°æ”¶æ–‚ã€‚


---

## ğŸ§© å››ã€GMM vs K-Means æ¯”è¼ƒ | GMM vs K-Means Comparison

| ç‰¹æ€§ Feature | K-Means | GMM |
|---------------|----------|------|
| ç¾¤é›†å½¢ç‹€ | çƒç‹€ (Spherical) | æ©¢åœ“å½¢ (Elliptical) |
| æ¨¡å‹é¡å‹ | å¹¾ä½•è·é›¢ (Distance-based) | æ©Ÿç‡æ¨¡å‹ (Probabilistic) |
| åˆ†ç¾¤æ–¹å¼ | ç¡¬åˆ†ç¾¤ (Hard) | è»Ÿåˆ†ç¾¤ (Soft) |
| ä½¿ç”¨æ¼”ç®—æ³• | Lloyd | EM |

---

## ğŸ äº”ã€Python å¯¦ä½œ 

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

# 1ï¸âƒ£ ç”Ÿæˆæ¨¡æ“¬è³‡æ–™ / Generate synthetic data
X, y_true = make_blobs(n_samples=500, centers=3, cluster_std=0.6, random_state=42)

# 2ï¸âƒ£ å»ºç«‹ GMM æ¨¡å‹ / Fit GMM model
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)
gmm.fit(X)
labels = gmm.predict(X)

# 3ï¸âƒ£ è¦–è¦ºåŒ– / Visualization
plt.figure(figsize=(8,6))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=30, cmap='viridis')
plt.title("Gaussian Mixture Model Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

---

## ğŸ“Š å…­ã€AIC / BIC æ¨¡å‹é¸æ“‡(Model Selection with AIC/BIC)

```python
K = range(1, 10)
aic_values, bic_values = [], []
for k in K:
    gmm = GaussianMixture(n_components=k, random_state=42)
    gmm.fit(X)
    aic_values.append(gmm.aic(X))
    bic_values.append(gmm.bic(X))

plt.plot(K, aic_values, marker='o', label='AIC')
plt.plot(K, bic_values, marker='s', label='BIC')
plt.legend()
plt.title("Model Selection using AIC/BIC")
plt.xlabel('Number of Components')
plt.ylabel('Score')
plt.show()
```

---

## ğŸŒ ä¸ƒã€Streamlit äº’å‹•ç‰ˆ(Streamlit Interactive Version)

å»ºç«‹ä¸€å€‹äº’å‹•å¼ GMM è¦–è¦ºåŒ–æ‡‰ç”¨ï¼š

```python
# gmm_streamlit_app.py
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from sklearn.datasets import make_blobs

st.title("ğŸ¨ Gaussian Mixture Model (GMM) Interactive Demo")

# Sidebar æ§åˆ¶é … / Sidebar controls
n_components = st.sidebar.slider("Number of Components (K)", 1, 10, 3)
std = st.sidebar.slider("Cluster Std Deviation", 0.1, 2.0, 0.6)

# ç”Ÿæˆè³‡æ–™ / Generate data
X, _ = make_blobs(n_samples=500, centers=n_components, cluster_std=std, random_state=42)

# æ¨¡å‹æ“¬åˆ / Fit GMM
gmm = GaussianMixture(n_components=n_components, random_state=42)
gmm.fit(X)
labels = gmm.predict(X)

# é¡¯ç¤ºçµæœ / Display results
fig, ax = plt.subplots(figsize=(7,5))
ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=30)
ax.set_title(f"GMM with {n_components} Components")
ax.set_xlabel("Feature 1")
ax.set_ylabel("Feature 2")
st.pyplot(fig)
```

åŸ·è¡Œï¼š
```bash
streamlit run gmm_streamlit_app.py
```

---

## å…«ã€å»¶ä¼¸æ‡‰ç”¨
- ç•°å¸¸åµæ¸¬ï¼ˆAnomaly Detectionï¼‰ï¼šä»¥ä½æ©Ÿç‡å€åŸŸç‚ºç•°å¸¸é»ã€‚
  - [ç¯„ä¾‹_é«˜æ–¯æ··åˆæ¨¡å‹_ç•°å¸¸åµæ¸¬](ç¯„ä¾‹_GMM_2.md) 
- èªéŸ³ç‰¹å¾µå»ºæ¨¡ï¼ˆMFCC ç‰¹å¾µçš„ GMMï¼‰
- å½±åƒé¡è‰²åˆ†å‰²
- ç”Ÿæˆå¼æ¨¡å‹åŸºç¤ï¼ˆå¦‚ VAEã€GMM-HMMï¼‰
  - https://medium.com/ml-note/gmm-hmm-69301f306b15 

## ğŸ§ª æ¸¬é©—é¡Œ | Quiz

1ï¸âƒ£ **GMM çš„æ ¸å¿ƒå‡è¨­æ˜¯ä»€éº¼ï¼Ÿ**  
â†’ è³‡æ–™ä¾†è‡ªå¤šå€‹é«˜æ–¯åˆ†ä½ˆçš„æ··åˆã€‚

2ï¸âƒ£ **AIC/BIC ç”¨æ–¼ä»€éº¼ï¼Ÿ**  
â†’ æ¨¡å‹é¸æ“‡ï¼Œæ±ºå®šæœ€ä½³ç¾¤é›†æ•¸ã€‚

3ï¸âƒ£ **GMM çš„åˆ†ç¾¤æ–¹å¼å±¬æ–¼å“ªä¸€é¡ï¼Ÿ**  
â†’ è»Ÿåˆ†ç¾¤ï¼ˆSoft Clusteringï¼‰ã€‚

---

## ğŸ“š ä¹ã€å»¶ä¼¸é–±è®€ | Further Reading
- Bishop, *Pattern Recognition and Machine Learning* (2006)  
- scikit-learn Docs: [https://scikit-learn.org/stable/modules/mixture.html](https://scikit-learn.org/stable/modules/mixture.html)  
- Murphy, *Machine Learning: A Probabilistic Perspective* (2012)

---

ğŸ’¾ **èªªæ˜**ï¼š

