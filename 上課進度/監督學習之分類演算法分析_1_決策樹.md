# Decision Tree ==> 可用在解決分類問題,也可用在解決回歸問題
- 決策樹的構建標準
  - A.分類問題 ==> [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
    - 1.信息增益（Information Gain）
      - 衡量選擇某一特徵後資料集的純度提升。
      - 計算公式為：
      - 其中 Entropy 是資料集的熵，用來衡量資料的不確定性。
    - 2.Gain ratio (吉尼獲利)
    - 3.Gini index (吉尼係數) |Gini Impurity (吉尼不純度)=>用於分類問題
      - 計算公式為：Gini()=1-sum (pi^2)
      - 其中 pi 是類別 i 的樣本占比。
      - 數字越大代表序列中的資料越混亂==>基尼指數越小，表示資料集越純淨。
  - B.回歸問題==>回歸樹 ==> [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)
    - 迴歸樹透過是 均方誤差（MSE) 或 MAE 來評估模型，並找出誤差最小的值作為樹的特徵選擇與切割點
      - 衡量預測值和真實值的差異。
      - MSE 越小，表示回歸樹的預測效果越好。
- 修剪樹(Tree Pruning technology)
  - https://www.tpointtech.com/machine-learning-decision-tree-classification-algorithm
  - 修剪是從樹中刪除不必要的節點以獲得最佳決策樹的過程。
  - 太大的樹會增加`過度擬合(overfitting)`的風險，而小樹可能無法捕獲數據集的所有重要特徵。
  - 因此，一種在不降低準確性的情況下減小學習樹大小的技術稱為`修剪(Tree Pruning)`。
  - 主要使用兩種類型的樹木修剪技術：
    - Cost Complexity Pruning
    - Reduced Error Pruning
- 決策樹演算法: [參考資料](https://www.geeksforgeeks.org/decision-tree-algorithms/)
  - ID3(Iterative Dichotomiser 3) ==> [介紹](https://www.geeksforgeeks.org/iterative-dichotomiser-3-id3-algorithm-from-scratch/)
  - C4.5
  - CART(Classification and Regression Trees)
  - CHAID (Chi-Square Automatic Interaction Detection)
  - MARS(Multivariate Adaptive Regression Splines)
  - Conditional Inference Trees
- 決策樹的優缺點
- 優點
  - 易於理解和解釋：決策樹的結構直觀，易於理解和解釋。
  - 處理多種資料類型：可以處理數值型和類別型資料。
  - 不需要資料標準化：決策樹不需要對資料進行標準化或歸一化處理。
- 缺點
  - 容易過擬合：決策樹容易過擬合，特別是在資料集較小或樹深度較大時。
  - 對雜訊敏感：決策樹對雜訊資料較為敏感，可能導致模型性能下降。
  - 不穩定：資料的小變化可能導致生成完全不同的樹。
## 範例1.
- https://ithelp.ithome.com.tw/articles/10271143
```python
from sklearn.tree import DecisionTreeClassifier

# 建立 DecisionTreeClassifier 模型
decisionTreeModel = DecisionTreeClassifier(criterion = 'entropy', max_depth=6, random_state=42)

# 訓練==>使用訓練資料訓練模型
decisionTreeModel.fit(train_reduced, y_train)

# 預測==>使用訓練資料預測分類
predicted = decisionTreeModel.predict(train_reduced)

# 計算準確率
accuracy = decisionTreeModel.score(train_reduced, y_train)
```
## 範例2.
```python
from sklearn.tree import DecisionTreeRegressor

# 建立 DecisionTreeRegressor 模型
decisionTreeModel = DecisionTreeRegressor(criterion = 'mse', max_depth=4, splitter='best', random_state=42)

# 訓練==>使用訓練資料訓練模型
decisionTreeModel.fit(x, y)

# 預測==>使用訓練資料預測
predicted=decisionTreeModel.predict(x)
```
# 參考資料
- https://medium.com/@MrBam44/decision-trees-91f61a42c724
