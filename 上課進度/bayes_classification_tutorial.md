# Bayes Classification Tutorial è´æ°å®šç†èˆ‡åˆ†é¡æ•™å­¸

---

## Part 1 | Bayes Theorem è´æ°å®šç†

### ğŸ§¬ Definition å®šç¾©
Bayes å®šç†ç”¨æ–¼èªªæ˜åœ¨è§€å¯Ÿåˆ°è³‡æ–™ X å¾Œï¼ŒæŸå€‹å‡è¨­ H ç‚ºçœŸçš„æ©Ÿç‡ã€‚

$$
P(H|X) = \frac{P(X|H) \cdot P(H)}{P(X)}
$$

å…¶ä¸­ï¼š
- \( P(H|X) \)ï¼šå¾Œé©—æ©Ÿç‡ (Posterior Probability)
- \( P(H) \)ï¼šå…ˆé©—æ©Ÿç‡ (Prior Probability)
- \( P(X|H) \)ï¼šä¼¼ç„¶åº¦ (Likelihood)
- \( P(X) \)ï¼šè³‡æ–™çš„ç¸½é«”æ©Ÿç‡

### ğŸ§® Intuition ç›´è¦ºæ„ç¾©
Bayes å®šç†å¯ä»¥ç”¨æ–¼æ›´æ–°æˆ‘å€‘å°ä¸€å€‹å‡è¨­çš„ä¿¡å¿ƒç¨‹åº¦ï¼š
> ã€Œåœ¨æ–°è³‡æ–™å‡ºç¾å¾Œï¼Œæˆ‘å€‘å°æŸå‡è¨­ç‚ºçœŸçš„ä¿¡å¿ƒæœƒæ€éº¼æ”¹è®Šï¼Ÿã€

---

## Part 2 | Bayesian Classification è´æ°åˆ†é¡

è©¦è‘—ä¾æ“šç‰¹å¾µ X = (x1, x2, ..., xn)ï¼Œåˆ¤æ–·æ‰€å±¬é¡åˆ¥ Ckã€‚

$$
\hat{C} = \arg\max_{C_k} P(C_k|X)
$$

ä½¿ç”¨ Bayes å®šç†ï¼š

$$
P(C_k|X) = \frac{P(X|C_k) \cdot P(C_k)}{P(X)}
$$

ç”±æ–¼ \( P(X) \) ç›¸åŒï¼Œå¯ç°¡åŒ–ç‚ºï¼š

$$
\hat{C} = \arg\max_{C_k} P(X|C_k) \cdot P(C_k)
$$

---

## Part 3 | Naive Bayes Classifier ç°¡å–®è´æ°åˆ†é¡å™¨

### ğŸ” Independence Assumption æ¢ä»¶ç¨ç«‹å‡è¨­

$$
P(X|C_k) = \prod_{i=1}^{n} P(x_i|C_k)
$$

ä»¥æ­¤å¯ä»¥å¤§å¹…é™ä½é‹ç®—è¤‡é›œåº¦ï¼Œæ˜¯ Naive Bayes çš„æ ¸å¿ƒç‰¹é»ã€‚

### ğŸ”¢ Common Types å¸¸è¦‹é¡å‹

| é¡å‹ | ç‰¹å¾µè³‡æ–™å‹æ…‹ | æ¢ä»¶åˆ†ä½ˆ | å¯¦ä¾‹ |
|------|----------------|--------------------|------|
| Gaussian NB | é€£çºŒ | æ­£æ…‹åˆ†ä½ˆ | Iris / Wine |
| Multinomial NB | é›œåˆè¨ˆæ•¸ | å¤šé …åˆ†ä½ˆ | æ–‡ä»¶åˆ†é¡ |
| Bernoulli NB | 0/1 | ä¼¯åŠªåˆ©åˆ†ä½ˆ | æ–‡å­—å‡ºç¾èˆ‡å¦ |

---

## Part 4 | Python Implementation (Gaussian Naive Bayes)

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Gaussian Naive Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
```

### ğŸŒ Decision Boundary Visualization

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import make_classification

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=4)

model = GaussianNB().fit(X, y)

# Create mesh grid
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
plt.title("Gaussian Naive Bayes Decision Boundary")
plt.show()
```

---

## Part 5 | Extended Chapter: Multinomial Naive Bayes æ–‡å­—åˆ†é¡

### ğŸ”¹ Example: IMDB Sentiment Analysis

```python
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
categories = ['rec.autos', 'rec.sport.baseball', 'talk.politics.mideast', 'sci.space']
newsgroups = fetch_20newsgroups(subset='train', categories=categories)

# TF-IDF vectorization
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(newsgroups.data)
y = newsgroups.target

# Train model
model = MultinomialNB()
model.fit(X, y)

# Predict
y_pred = model.predict(X)

# Evaluate
print("Accuracy:", accuracy_score(y, y_pred))
print("Classification Report:\n", classification_report(y, y_pred, target_names=categories))
```

### ğŸ”¹ View Top Keywords per Class

```python
import numpy as np

feature_names = np.array(vectorizer.get_feature_names_out())
for i, category in enumerate(categories):
    top10 = np.argsort(model.coef_[i])[-10:]
    print(f"\nTop keywords for class '{category}':")
    print(feature_names[top10])
```

---

## Part 6 | Pros & Cons å„ªç¼ºé»åˆ†æ

| å„ªé» | ç¼ºé» |
|------|------|
| è¨ˆç®—æ•ˆç‡é«˜ï¼Œé©åˆå¤§é‡è³‡æ–™ | æ¢ä»¶ç¨ç«‹å‡è¨­éæ–° |
| æ¨¡å‹ç°¡å–®ã€å¯è§£é‡‹æ€§é«˜ | ç‰¹å¾µç›¸é—œæ€§é«˜æ™‚æº–ç¢ºç‡é™ä½ |
| é©ç”¨æ–¼æ–‡å­—åˆ†é¡ã€åƒåœ¾éƒµä»¶æª¢æ¸¬ | é€£çºŒè³‡æ–™éœ€å‡è¨­åˆ†ä½ˆ |

---

## Part 7 | Applications æ‡‰ç”¨ç¯„ä¾‹

| é ˜åŸŸ | æ‡‰ç”¨ç¯„ä¾‹ |
|-----------|----------------|
| åƒåœ¾éƒµä»¶éæ¿¾ | åˆ¤æ–·éƒµä»¶æ˜¯å¦ç‚ºåƒåœ¾ |
| æ–‡ä»¶åˆ†é¡ | è‡ªå‹•æ–°èåˆ†é¡ |
| æƒ…æ„Ÿåˆ†æ | åˆ¤æ–·å½±è©•æ­£è² é¢ |
| é†«å­¸è¯Šæ–· | æ ¹æ“šç—‡ç‹€é æ¸¬ç–¾ç—…é¡å‹ |

---

## Part 8 | Practical Notes å¯¦å‹™å»ºè­°

1. å¦‚æœç‰¹å¾µä¸ç¨ç«‹ï¼Œå¯è€ƒæ…® **ComplementNB** æˆ–æ··åˆå‹æ¨¡å‹ã€‚  
2. å¯èˆ‡ **TF-IDF + Feature Selection** çµåˆä»¥æå‡æº–ç¢ºç‡ã€‚  
3. åœ¨å¤§è¦æ¨¡ NLP è³‡æ–™ä¸Šï¼Œä»ç„¶æ˜¯æ•ˆç‡æœ€é«˜çš„åŸºç´ åˆ†é¡æ–¹æ³•ä¹‹ä¸€ã€‚

---

âœ¨ **End of Tutorial** â€” `Bayes_Classification_Tutorial.md`

